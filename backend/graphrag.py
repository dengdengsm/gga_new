import os
import chromadb
import torch
import json
import uuid
import networkx as nx
import concurrent.futures
from typing import List, Tuple, Set, Dict, Any
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from Agent import deepseek_agent, Message

class LightGraphRAG:
    def __init__(self, persist_dir: str = "./.local_graph_db", prompt_dir: str = "./prompt/graphrag"):
        """
        åˆå§‹åŒ–è½»é‡çº§ GraphRAG V3.1 (Backbone-Fragment Optimization with Safety Limits)
        Updates:
        - Prompt Hardening: Strict limits on output size (Max 20 ops).
        - Backbone Protection: Explicit constraints against modifying core nodes.
        """
        print("--- åˆå§‹åŒ– LightGraphRAG V3.1 (Backbone-Centric Safe Mode) ---")
        
        self.prompt_dir = prompt_dir
        os.makedirs(self.prompt_dir, exist_ok=True) 
        
        # æ£€æŸ¥è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.encoder = SentenceTransformer("BAAI/bge-m3", device=device)
        
        self.client = chromadb.PersistentClient(path=persist_dir)
        
        self.node_collection = self.client.get_or_create_collection(
            name="graph_nodes",
            metadata={"hnsw:space": "cosine"}
        )

        self.chunk_collection = self.client.get_or_create_collection(
            name="graph_chunks",
            metadata={"hnsw:space": "cosine"}
        )
        
        self.graph_path = os.path.join(persist_dir, "knowledge_graph.json")
        self.graph = nx.Graph()
        self._load_graph()
        
        self.extractor = deepseek_agent(model_name="deepseek-chat")

    def _load_graph(self):
        if os.path.exists(self.graph_path):
            with open(self.graph_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.graph = nx.node_link_graph(data)
        else:
            os.makedirs(os.path.dirname(self.graph_path), exist_ok=True)

    def _save_graph(self):
        data = nx.node_link_data(self.graph)
        with open(self.graph_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)

    def _load_prompt_content(self, prompt_file: str, default_content: str = "") -> str:
        full_path = os.path.join(self.prompt_dir, prompt_file)
        if not os.path.exists(full_path) and default_content:
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(default_content)
            return default_content
        elif os.path.exists(full_path):
            with open(full_path, 'r', encoding='utf-8') as f:
                return f.read()
        return default_content

    def _read_text_file(self, file_path: str) -> str:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            print("UTF-8 decode failed, trying latin-1...")
            with open(file_path, 'r', encoding='latin-1') as f:
                return f.read()

    def _add_edge_safe(self, src: str, tgt: str, relation: str, source_id: str, summary: str):
        if self.graph.has_edge(src, tgt):
            edge_data = self.graph[src][tgt]
            if 'evidence' not in edge_data: edge_data['evidence'] = []
            
            exists = any(e.get('chunk_id') == source_id for e in edge_data['evidence'])
            if not exists:
                edge_data['evidence'].append({"chunk_id": source_id, "summary": summary})
        else:
            self.graph.add_edge(src, tgt, relation=relation, evidence=[{"chunk_id": source_id, "summary": summary}])

    # --- Phase 1 ---
    def _extract_chunk_content(self, text_chunk: str, system_prompt: str) -> Dict[str, Any]:
        messages = [{"role": "user", "content": f"Text Fragment:\n{text_chunk}"}]
        try:
            response = self.extractor.chat(messages, system_prompt=system_prompt, json_mode=True)
            return json.loads(response)
        except Exception as e:
            print(f"  [Error] åŸºç¡€æå–å¤±è´¥: {e}")
            return {"summary": "", "triples": []}

    # --- Phase 2 ---
    def _refine_adjacent_logic(self, chunk_curr: str, chunk_next: str, 
                             curr_id: str, next_id: str, 
                             prompt_text: str,
                             global_summaries: List[str],
                             known_triples: List[List[str]]) -> List[List[str]]:
        
        summary_context_str = "\n".join([f"- {i+1}. {s}" for i, s in enumerate(global_summaries) if s])
        known_triples_str = "; ".join([f"({t[0]}, {t[1]}, {t[2]})" for t in known_triples])
        
        user_msg = f"""
        === Global Context (Story Flow) ===
        {summary_context_str}

        === Already Extracted Facts (Do NOT Repeat) ===
        {known_triples_str}
        
        === Target Fragments ===
        [Fragment A (Preceding)]:
        {chunk_curr}
        
        [Fragment B (Following)]:
        {chunk_next}
        
        === Task ===
        Based on the Global Context, analyze the logical gap between Fragment A and Fragment B.
        1. Identify *implicit* connections that bridge A and B.
        2. Ignore facts already listed in "Already Extracted Facts".
        3. Extract ONLY new bridging relationships.
        
        Output JSON: {{ "new_edges": [["EntityInA", "connects_to", "EntityInB"]] }}
        """
        
        try:
            response = self.extractor.chat([{"role": "user", "content": user_msg}], system_prompt=prompt_text, json_mode=True)
            data = json.loads(response)
            new_edges = data.get("new_edges", [])
            if new_edges:
                print(f"  [ç¼åˆ] æ£€æµ‹åˆ° {len(new_edges)} æ¡æ½œåœ¨å…³ç³» (Chunks {curr_id.split('_')[1]}-{next_id.split('_')[1]})")
            return new_edges
        except Exception as e:
            print(f"  [Refine Error] é€»è¾‘ç¼åˆå¤±è´¥: {e}")
            return []

    # --- Phase 3 (Global Inference) ---
    def _infer_global_relationships(self, all_contents: List[Dict[str, Any]], prompt_text: str):
        if not all_contents: return
        print(f"\nâš¡ æ­£åœ¨è¿›è¡Œå…¨å±€é€»è¾‘æ¨å¯¼ (å»ºç«‹éª¨æ¶è¿æ¥)...")
        
        facts_lines = []
        for item in all_contents[:60]: 
            summ = item.get('summary', '')
            triples_str = "; ".join([f"{t[0]}-{t[1]}->{t[2]}" for t in item.get('triples', [])])
            if summ or triples_str:
                facts_lines.append(f"- {summ} | {triples_str}")

        facts_text = "\n".join(facts_lines)
        messages = [{"role": "user", "content": f"Fragmented Fact List:\n{facts_text}\n\nPlease infer global logic:"}]

        try:
            response = self.extractor.chat(messages, system_prompt=prompt_text)
            for line in response.strip().split('\n'):
                parts = line.split('|')
                if len(parts) == 3:
                    src, rel, tgt = parts[0].strip(), parts[1].strip(), parts[2].strip()
                    if src and rel and tgt:
                        self._add_edge_safe(src, tgt, rel, source_id="global_inference", summary="Global Logic Inference")
        except Exception as e:
            print(f"å…¨å±€æ¨å¯¼å¤±è´¥: {e}")

    # --- Phase 4 (Backbone-Fragment Optimization) ---
    def _optimize_graph_structure(self, prompt_text: str, max_iterations: int = 10):
        """
        åŸºäºä¸»å¹²(Backbone)å’Œç¢ç‰‡(Fragment)çš„å›¾ä¼˜åŒ–ç­–ç•¥ã€‚
        1. æå–æœ€å¤§çš„è¿é€šåˆ†é‡ä½œä¸º Backbone (ReadOnly Context)ã€‚
        2. æå–å…¶ä½™å°çš„è¿é€šåˆ†é‡ä½œä¸º Fragments (Target)ã€‚
        3. è®© LLM å†³å®š Fragments æ˜¯åˆ é™¤(Noise)ã€åˆå¹¶(Merge)è¿˜æ˜¯è¿æ¥(Connect)åˆ° Backboneã€‚
        """
        print(f"\nâš¡ å¯åŠ¨ä¸»å¹²-ç¢ç‰‡ä¼˜åŒ– (Backbone-Centric Rewiring)...")
        
        for i in range(max_iterations):
            # 1. æå–è¿é€šåˆ†é‡
            components = list(nx.connected_components(self.graph))
            if len(components) <= 1:
                print("  âœ… å›¾è°±å·²å®Œå…¨è¿é€šï¼Œç»“æŸä¼˜åŒ–ã€‚")
                break
            
            # æŒ‰èŠ‚ç‚¹æ•°é‡æ’åºï¼Œæœ€å¤§çš„ä¸ºä¸»å¹²
            components.sort(key=len, reverse=True)
            backbone_nodes = components[0]
            fragment_nodes = set().union(*components[1:])
            
            num_fragments = len(components) - 1
            print(f"  [Iter {i+1}] ä¸»å¹²èŠ‚ç‚¹: {len(backbone_nodes)} | ç¢ç‰‡åŒºåŸŸ: {num_fragments} ä¸ª | å¾…å¤„ç†èŠ‚ç‚¹: {len(fragment_nodes)}")

            # 2. å‡†å¤‡ä¸Šä¸‹æ–‡ (Backbone Edges)
            # ä¸ºäº†èŠ‚çœ Tokenï¼Œåªå– Backbone ä¸­åº¦æ•°è¾ƒé«˜çš„èŠ‚ç‚¹çš„è¾¹ï¼Œæˆ–è€…éšæœºé‡‡æ ·
            backbone_subgraph = self.graph.subgraph(backbone_nodes)
            backbone_edges = []
            # ç®€å•é‡‡æ ·ç­–ç•¥ï¼šå–å‰ 500 æ¡è¾¹
            for u, v, d in list(backbone_subgraph.edges(data=True))[:500]:
                backbone_edges.append(f"{u} --[{d.get('relation','related')}]--> {v}")
            backbone_str = "\n".join(backbone_edges)

            # 3. å‡†å¤‡ç›®æ ‡æ•°æ® (Fragment Edges)
            fragment_subgraph = self.graph.subgraph(fragment_nodes)
            fragment_edges = []
            # ç¢ç‰‡é€šå¸¸è¾ƒå°ï¼Œå°½å¯èƒ½å…¨éƒ¨æä¾›
            for u, v, d in list(fragment_subgraph.edges(data=True))[:300]:
                fragment_edges.append(f"{u} --[{d.get('relation','related')}]--> {v}")
            
            # å¦‚æœç¢ç‰‡æ²¡æœ‰å†…éƒ¨è¾¹ï¼ˆå…¨æ˜¯å­¤ç«‹ç‚¹ï¼‰ï¼Œåˆ™åˆ—å‡ºèŠ‚ç‚¹
            fragment_isolates = [n for n in fragment_nodes if self.graph.degree(n) == 0]
            
            fragment_str = "\n".join(fragment_edges)
            if fragment_isolates:
                fragment_str += "\nIsolated Nodes: " + ", ".join(fragment_isolates[:50])

            if not fragment_str.strip():
                # åªæœ‰å­¤ç«‹ç‚¹ä¸”å·²è¢«å¤„ç†ï¼Œæˆ–è€…æ²¡æœ‰è¾¹ä¿¡æ¯ï¼Œå°è¯•ç›´æ¥ç§»é™¤å¾®å°å­¤ç«‹ç‚¹
                print("  -> ç¢ç‰‡åŒºåŸŸæ— æœ‰æ•ˆè¾¹ä¿¡æ¯ï¼Œæ¸…ç†å¾®å°å­¤ç«‹ç‚¹ã€‚")
                self.graph.remove_nodes_from(list(nx.isolates(self.graph)))
                continue

            # 4. æ„é€  Prompt (Updated constraints)
            user_msg = f"""
            === The Main Knowledge Backbone (Context, READ ONLY) ===
            This is the largest connected component representing the core logic.
            **STRICT RULE: YOU CANNOT DELETE OR RENAME NODES IN THE BACKBONE.**
            {backbone_str}

            === Disconnected Fragments (Target for Optimization) ===
            These are isolated components/edges. Analyze them against the Backbone.
            {fragment_str}

            === Task ===
            For each Entity or Edge in the Fragments, decide its fate:
            1. **DELETE**: If it looks like noise, typos, or irrelevant data.
            2. **MERGE**: If a fragment entity is a synonym of a Backbone entity. (e.g., "AI" -> "Artificial Intelligence").
               *IMPORTANT*: 'source' must be from Fragment, 'target' must be from Backbone.
            3. **CONNECT**: If the fragment represents valid logic but is missing a link to the Backbone. Create a NEW edge.
            4. **IGNORE**: If it is valid but distinct topic, or you are unsure. DO NOT hallucinate connections.

            === Constraints ===
            1. **Max Output**: Return AT MOST 20 operations to prevent response truncation. Prioritize the most high-confidence changes.
            2. **Preserve Backbone**: Do not output operations that modify the Backbone structure itself (only attach to it).

            === Output Format (JSON) ===
            {{
                "operations": [
                    {{ "type": "DELETE", "nodes": ["noise_node_A"] }},
                    {{ "type": "MERGE", "source": "fragment_node_B", "target": "backbone_node_C" }},
                    {{ "type": "CONNECT", "source": "fragment_node_D", "target": "backbone_node_E", "relation": "rel_name" }}
                ]
            }}
            """

            try:
                # ä½¿ç”¨ resolve_prompt æˆ–é»˜è®¤
                system_p = prompt_text if prompt_text else "You are a Knowledge Graph optimizor."
                response = self.extractor.chat([{"role": "user", "content": user_msg}], system_prompt=system_p, json_mode=True)
                data = json.loads(response)
                ops = data.get("operations", [])
                
                if not ops:
                    print("  -> LLM æœªæå‡ºä¿®æ”¹å»ºè®®ã€‚")
                    if i > 2: break 
                    continue

                changed = False
                for op in ops:
                    op_type = op.get("type", "").upper()
                    
                    if op_type == "DELETE":
                        nodes_to_del = op.get("nodes", [])
                        for n in nodes_to_del:
                            # é¢å¤–ä¿æŠ¤ï¼šç¡®ä¿åªåˆ é™¤ Fragment ä¸­çš„èŠ‚ç‚¹ï¼Œä¸åˆ é™¤ Backbone
                            if n in backbone_nodes:
                                print(f"    [SKIP] è¯•å›¾åˆ é™¤ä¸»å¹²èŠ‚ç‚¹ {n}ï¼Œæ“ä½œå·²æ‹¦æˆªã€‚")
                                continue
                            if self.graph.has_node(n):
                                self.graph.remove_node(n)
                                changed = True
                        if nodes_to_del: print(f"    [DEL] åˆ é™¤å»ºè®®: {nodes_to_del}")

                    elif op_type == "MERGE":
                        src = op.get("source")
                        tgt = op.get("target")
                        # é¢å¤–ä¿æŠ¤ï¼šç¡®ä¿ src æ˜¯ Fragment, tgt æ˜¯ Backbone (æˆ–è‡³å°‘åœ¨å›¾ä¸­)
                        if src and tgt and self.graph.has_node(src) and self.graph.has_node(tgt):
                            if src in backbone_nodes and tgt not in backbone_nodes:
                                # å¦‚æœ LLM æåäº†æ–¹å‘ï¼Œä¸” tgt æ˜¯ fragmentï¼Œæˆ‘ä»¬å°è¯•åè½¬é€»è¾‘ï¼Ÿ
                                # æˆ–è€…ä¸¥æ ¼æ‹’ç»ã€‚è¿™é‡Œé€‰æ‹©ä¸¥æ ¼æ‹’ç»ä»¥é˜²æ­¢ç ´åä¸»å¹²ã€‚
                                print(f"    [SKIP] æ‹’ç»å°†ä¸»å¹²èŠ‚ç‚¹ {src} åˆå¹¶å…¥ {tgt}ã€‚")
                                continue
                                
                            # å°† src çš„è¿æ¥ç§»åˆ° tgt
                            for nbr in list(self.graph.neighbors(src)):
                                if nbr == tgt: continue
                                edge_data = self.graph[src][nbr]
                                self._add_edge_safe(tgt, nbr, edge_data.get('relation','merged'), "merge_op", "Merged synonym")
                            self.graph.remove_node(src)
                            changed = True
                            print(f"    [MERGE] åˆå¹¶: {src} -> {tgt}")

                    elif op_type == "CONNECT":
                        src = op.get("source")
                        tgt = op.get("target")
                        rel = op.get("relation", "related")
                        if src and tgt and self.graph.has_node(src) and self.graph.has_node(tgt):
                            self._add_edge_safe(src, tgt, rel, "connect_op", "Logic Gap Filled")
                            changed = True
                            print(f"    [CONNECT] è¿é€š: {src} --[{rel}]--> {tgt}")

                if not changed:
                    print("  -> å»ºè®®æ“ä½œæœªå¼•èµ·å›¾è°±å®è´¨å˜åŒ–ã€‚")
                    break

            except Exception as e:
                print(f"  [Iter {i+1} Error] ä¼˜åŒ–å‡ºé”™: {e}")
                break

        # æœ€åæ¸…ç†æ‰€æœ‰ä»ç„¶å­¤ç«‹çš„å•ç‚¹ï¼ˆé€šå¸¸æ˜¯æ“ä½œåé—ç•™çš„ï¼‰
        final_isolates = list(nx.isolates(self.graph))
        if final_isolates:
            self.graph.remove_nodes_from(final_isolates)
            print(f"  [Cleanup] æœ€ç»ˆæ¸…ç† {len(final_isolates)} ä¸ªå­¤ç«‹èŠ‚ç‚¹ã€‚")

    # --- ä¸»æµç¨‹ ---
    def build_graph(self, file_path: str, 
                    extract_prompt: str = "common_pmp.md",
                    refine_prompt: str = "refine_pmp.md",
                    resolve_prompt: str = "resolve_pmp.md",
                    infer_prompt: str = "infer_pmp.md"):
        
        p_extract = self._load_prompt_content(extract_prompt, default_content="Extract triples and summary JSON.")
        p_refine = self._load_prompt_content(refine_prompt, default_content="Find logic between two chunks JSON.")
        p_resolve = self._load_prompt_content(resolve_prompt, default_content="Backbone-Fragment Optimization JSON.")
        p_infer = self._load_prompt_content(infer_prompt, default_content="Infer global logic Entity|Rel|Entity.")

        try:
            text = self._read_text_file(file_path)
        except Exception as e:
            print(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return

        print(f"æ­£åœ¨æ„å»ºå›¾è°±: {file_path}")
        splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
        chunks = splitter.split_text(text)
        if not chunks: return
        
        base_name = os.path.basename(file_path)
        chunk_ids = [f"{base_name}_{i}_{str(uuid.uuid4())[:8]}" for i in range(len(chunks))]
        
        print(f"æ­£åœ¨å‘é‡åŒ– {len(chunks)} ä¸ªç‰‡æ®µ...")
        try:
            embeddings = self.encoder.encode(chunks, normalize_embeddings=True).tolist()
            self.chunk_collection.add(
                ids=chunk_ids,
                documents=chunks,
                metadatas=[{"source": file_path, "index": i} for i in range(len(chunks))],
                embeddings=embeddings
            )
        except Exception as e:
            print(f"å‘é‡åŒ–å¤±è´¥: {e}")

        # Phase 1: Concurrent Extraction
        print(f"Phase 1: å¯åŠ¨åŸºç¡€æå– (Tasks: {len(chunks)})...")
        file_contents = [None] * len(chunks)
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            future_to_index = {
                executor.submit(self._extract_chunk_content, chunk, p_extract): i 
                for i, chunk in enumerate(chunks)
            }
            for future in concurrent.futures.as_completed(future_to_index):
                i = future_to_index[future]
                file_contents[i] = future.result() or {}

        print("å†™å…¥åˆæ­¥å›¾è°±æ•°æ®...")
        all_summaries = [] 
        for i, data in enumerate(file_contents):
            chunk_id = chunk_ids[i]
            summary = data.get("summary", "No Summary")
            all_summaries.append(summary)
            for item in data.get("triples", []):
                if len(item) < 3: continue
                src, rel, tgt = item[:3]  # åªå–å‰3ä¸ªï¼Œå¿½ç•¥å¤šä½™çš„
                self._add_edge_safe(src, tgt, rel, source_id=chunk_id, summary=summary)

        # Phase 2: Concurrent Refinement
        if len(chunks) > 1:
            print(f"Phase 2: å¹¶å‘é€»è¾‘ç¼åˆ (Tasks: {len(chunks)-1})...")
            refine_results = [[] for _ in range(len(chunks) - 1)]
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
                future_to_index = {}
                for i in range(len(chunks) - 1):
                    current_triples = file_contents[i].get("triples", [])
                    next_triples = file_contents[i+1].get("triples", [])
                    known_triples = current_triples + next_triples
                    
                    future = executor.submit(
                        self._refine_adjacent_logic,
                        chunk_curr=chunks[i], 
                        chunk_next=chunks[i+1], 
                        curr_id=chunk_ids[i], 
                        next_id=chunk_ids[i+1], 
                        prompt_text=p_refine,
                        global_summaries=all_summaries, 
                        known_triples=known_triples     
                    )
                    future_to_index[future] = i
                
                for future in concurrent.futures.as_completed(future_to_index):
                    idx = future_to_index[future]
                    try:
                        new_edges = future.result()
                        refine_results[idx] = new_edges
                    except Exception as e:
                        print(f"Task {idx} failed: {e}")

            print("æ­£åœ¨æ•´åˆç¼åˆç»“æœ...")
            count_new = 0
            for i, new_edges in enumerate(refine_results):
                if not new_edges: continue
                curr_id = chunk_ids[i]
                next_id = chunk_ids[i+1]
                for edge in new_edges:
                    if not isinstance(edge, list) or len(edge) < 3: continue
                    src, rel, tgt = edge[:3]
                    self._add_edge_safe(src, tgt, rel, source_id=f"link_{curr_id}_{next_id}", summary="Adjacent Logic Inference")
                    count_new += 1
            print(f"âœ¨ é€»è¾‘ç¼åˆå®Œæˆï¼Œæ–°å¢ {count_new} æ¡è·¨å—é€»è¾‘ã€‚")

        # Phase 3: Global Inference (Backbone)
        print(f"Phase 3: å®è§‚é€»è¾‘æ¨å¯¼...")
        valid_contents = [c for c in file_contents if c]
        self._infer_global_relationships(valid_contents, p_infer)

        # Phase 4: Backbone-Fragment Optimization
        print(f"Phase 4: ä¸»å¹²-ç¢ç‰‡å®ä½“å¯¹é½ä¼˜åŒ–...")
        self._optimize_graph_structure(p_resolve, max_iterations=10)

        # Final Indexing
        final_nodes = list(self.graph.nodes())
        if final_nodes:
            print(f"æ­£åœ¨æ›´æ–°èŠ‚ç‚¹ç´¢å¼• ({len(final_nodes)} nodes)...")
            try:
                node_embeddings = self.encoder.encode(final_nodes, normalize_embeddings=True).tolist()
                self.node_collection.upsert(
                    ids=[f"node_{n}" for n in final_nodes],
                    documents=final_nodes,
                    embeddings=node_embeddings
                )
            except Exception as e:
                print(f"èŠ‚ç‚¹ç´¢å¼•æ›´æ–°å¤±è´¥: {e}")

        self._save_graph()
        print(f"âœ¨ å›¾è°±æ„å»ºå®Œæˆã€‚Nodes: {self.graph.number_of_nodes()}, Edges: {self.graph.number_of_edges()}")

    def search(self, query: str, top_k: int = 3) -> str:
        print(f"\n[Search] æ­£åœ¨æ£€ç´¢: {query}")
        
        query_vec = self.encoder.encode([query], normalize_embeddings=True).tolist()
        node_results = self.node_collection.query(query_embeddings=query_vec, n_results=5)
        found_nodes = node_results['documents'][0] if node_results['documents'] else []
        
        if not found_nodes:
            return "No related entities found."
            
        print(f"  -> å‘½ä¸­å®ä½“: {found_nodes}")

        logic_lines = []
        visited_edges = set()

        for node in found_nodes:
            if self.graph.has_node(node):
                edges = self.graph.edges(node, data=True)
                for u, v, data in edges:
                    edge_sig = tuple(sorted([u, v]))
                    if edge_sig in visited_edges: continue
                    
                    rel = data.get('relation', 'related_to')
                    evidence_list = data.get('evidence', [])
                    summaries = list(set([e['summary'] for e in evidence_list]))[:2] 
                    summary_text = f" (Context: {'; '.join(summaries)})" if summaries else ""
                    
                    logic_lines.append(f"- {u} --[{rel}]--> {v}{summary_text}")
                    visited_edges.add(edge_sig)

        chunk_results = self.chunk_collection.query(
            query_embeddings=query_vec,
            n_results=top_k
        )
        
        top_chunks = chunk_results['documents'][0] if chunk_results['documents'] else []
        top_ids = chunk_results['ids'][0] if chunk_results['ids'] else []
        
        source_text_blocks = []
        for i, (txt, cid) in enumerate(zip(top_chunks, top_ids)):
            source_text_blocks.append(f"Source Fragment {i+1} (ID: {cid}):\n{txt}")

        final_context = "### Knowledge Graph Paths\n" + "\n".join(logic_lines) + "\n\n"
        final_context += "### Source References\n" + "\n".join(source_text_blocks)
        
        return final_context

    def clear_db(self):
        try:
            self.client.delete_collection("graph_nodes")
            self.client.delete_collection("graph_chunks")
        except:
            pass
        
        self.node_collection = self.client.get_or_create_collection(name="graph_nodes", metadata={"hnsw:space": "cosine"})
        self.chunk_collection = self.client.get_or_create_collection(name="graph_chunks", metadata={"hnsw:space": "cosine"})
        
        self.graph.clear()
        self._save_graph()
        print("æ•°æ®åº“å·²é‡ç½®ã€‚")
    def reload_db(self, new_persist_dir: str):
        """
        åŠ¨æ€åˆ‡æ¢ GraphRAG çš„æŒä¹…åŒ–å­˜å‚¨è·¯å¾„ (ç”¨äºé¡¹ç›®åˆ‡æ¢)
        """
        print(f"ğŸ”„ [GraphRAG] æ­£åœ¨åˆ‡æ¢æ•°æ®åº“è·¯å¾„è‡³: {new_persist_dir}")
        
        # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(new_persist_dir, exist_ok=True)

        # 2. é‡æ–°è¿æ¥ ChromaDB
        # ç›´æ¥è¦†ç›– self.client å®ä¾‹å³å¯æŒ‡å‘æ–°è·¯å¾„
        self.client = chromadb.PersistentClient(path=new_persist_dir)
        
        # 3. é‡æ–°ç»‘å®šé›†åˆ (Collection)
        # æŒ‡é’ˆæŒ‡å‘æ–° DB ä¸­çš„é›†åˆ
        self.node_collection = self.client.get_or_create_collection(
            name="graph_nodes",
            metadata={"hnsw:space": "cosine"}
        )

        self.chunk_collection = self.client.get_or_create_collection(
            name="graph_chunks",
            metadata={"hnsw:space": "cosine"}
        )
        
        # 4. é‡æ–°åŠ è½½ NetworkX å›¾è°±ç»“æ„
        self.graph_path = os.path.join(new_persist_dir, "knowledge_graph.json")
        self.graph = nx.Graph() # å…ˆæ¸…ç©ºå†…å­˜ä¸­çš„æ—§å›¾
        self._load_graph()      # ä»æ–°è·¯å¾„åŠ è½½ (è‹¥æ— æ–‡ä»¶åˆ™ä¸ºç©ºå›¾)
        
        print(f"âœ… [GraphRAG] åˆ‡æ¢å®Œæˆã€‚å½“å‰èŠ‚ç‚¹æ•°: {self.graph.number_of_nodes()}")

if __name__ == "__main__":
    kg = LightGraphRAG()
    
    text = """
    # æ¨¡å—Aï¼šæ•°æ®é‡‡é›†
    é‡‡é›†å™¨ä»ç½‘ç»œæŠ“å–æ•°æ®ï¼Œå¹¶å­˜å…¥MongoDBä¸­ã€‚
    
    # æ¨¡å—Bï¼šæ•°æ®é¢„å¤„ç†
    Mongoæ•°æ®åº“ä¸­çš„æ•°æ®è¢«è¯»å–åï¼Œè¿›è¡Œæ¸…æ´—å’Œå»é‡ã€‚
    
    # æ¨¡å—Cï¼šç‰¹å¾å·¥ç¨‹
    æ¸…æ´—åçš„æ•°æ®è¢«é€å…¥ç‰¹å¾æå–å™¨ï¼Œç”Ÿæˆå‘é‡ã€‚
    """
    
    with open("test_v3.txt", "w", encoding='utf-8') as f: f.write(text)
    kg.clear_db()
    kg.build_graph("test_v3.txt")
    print("\n--- æµ‹è¯•æœç´¢ ---")
    print(kg.search("æ•°æ®åº“"))