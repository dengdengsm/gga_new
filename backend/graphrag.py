import os
import json
import networkx as nx
import numpy as np
import pickle
import torch
import heapq
import time
import re
from collections import Counter, defaultdict
from typing import List, Dict, Any, Optional, Set, Tuple
from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# å‡è®¾ Agent å·²ç»æŒ‰ç…§ä¹‹å‰çš„æ¥å£å®ç°å¥½
from Agent import deepseek_agent, qwen_doc_agent
import logging

# å…³é—­ httpx (OpenAI/DeepSeek åº•å±‚é€šè®¯åº“) çš„ INFO æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)

# å¦‚æœè¿˜æœ‰å…¶ä»–å¹²æ‰°ï¼Œå¯ä»¥å°è¯•å…³é—­è¿™äº›å¸¸è§åº“çš„æ—¥å¿—
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
# ==========================================
# Configuration & Utils
# ==========================================

MAX_RETRIES = 2
RETRY_DELAY = 1

def clean_json_response(response: str) -> str:
    """æ¸…æ´— LLM å¯èƒ½è¿”å›çš„ markdown æ ‡è®°"""
    if "```json" in response:
        response = response.split("```json")[1].split("```")[0]
    elif "```" in response:
        response = response.split("```")[1].split("```")[0]
    return response.strip()

# ==========================================
# LightGraphRAG V6.0 (Pyramid Architecture)
# ==========================================

class LightGraphRAG:
    """
    LightGraphRAG V6.0: The "Pyramid" Architecture
    
    Structure:
    - Layer 1: Global Backbone (Qwen-Long) -> Defines the "Skeleton".
    - Layer 2: Intermediate Bridge (DeepSeek + Large Chunks) -> Connects flesh to skeleton.
    - Layer 3: Local Detail (DeepSeek + Small Chunks) -> Adds capillary details (Graph-Constrained).
    
    Features:
    - Strict Provenance Tracking (Node -> Set[ChunkIDs]).
    - Intent-Driven Graph Construction.
    - Connectivity-Enforced Local Extraction.
    """
    
    def __init__(self, persist_dir: str = "./graph_db_v6"):
        print("--- åˆå§‹åŒ– LightGraphRAG V6.0 (Pyramid Architecture) ---")
        self.persist_dir = persist_dir
        os.makedirs(self.persist_dir, exist_ok=True)
        
        # 1. Graph Data Structure
        # Nodes: id, description, type (backbone/intermediate/leaf), source_chunks (Set), importance (float)
        # Edges: src, dst, description, weight, source_chunk_id
        self.graph = nx.DiGraph()
        self.graph_version = 0

        self.lock = threading.Lock() # âœ… æ–°å¢ï¼šå…¨å±€å›¾å†™å…¥é”
        
        # 2. Chunk Storage
        # æˆ‘ä»¬ç»´æŠ¤ä¸¤å¥—åˆ‡ç‰‡ï¼šBig Chunks ç”¨äºå±‚çº§2ï¼ŒSmall Chunks ç”¨äºå±‚çº§3å’Œæœ€ç»ˆæ£€ç´¢
        self.small_chunks = [] # List[Dict]
        self.big_chunks = []   # List[Dict]
        
        # 3. Embedding Model
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ğŸš€ Loading Embedding Model (BGE-M3) on {device}...")
        try:
            self.embed_model = SentenceTransformer("BAAI/bge-m3", device=device)
        except Exception as e:
            print(f"   âŒ Embedding Model Load Failed: {e}")
            self.embed_model = None
            
        # 4. Initialize Agents
        print("   ğŸ¤– Initializing LLM Agents...")
        self.local_extractor = deepseek_agent(model_name="deepseek-chat") # 64k context
        self.global_planner = qwen_doc_agent(model_name="qwen-long")      # Long context
        
        # 5. Load State
        self.load_graph()

    # =========================================================================
    # Phase 0: Pre-processing
    # =========================================================================

    def _chunk_document_dual_layer(self, doc_path: str):
        """åŒæ—¶ç”Ÿæˆå¤§ç²’åº¦(4000)å’Œå°ç²’åº¦(600)åˆ‡ç‰‡"""
        try:
            with open(doc_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            base_id = os.path.basename(doc_path)
            total_len = len(text)
            
            # 1. Big Chunks (For Layer 2: Intermediate Structure)
            # Size: 1500, Overlap: 200
            self.big_chunks = []
            chunk_size_big = 1500
            for i in range(0, total_len, chunk_size_big - 200):
                self.big_chunks.append({
                    "id": f"big_{i//chunk_size_big}",
                    "text": text[i : i + chunk_size_big],
                    "source": base_id
                })
                
            # 2. Small Chunks (For Layer 3 & Retrieval)
            # Size: 300, Overlap: 100
            self.small_chunks = []
            chunk_size_small = 500
            for i in range(0, total_len, chunk_size_small - 100):
                self.small_chunks.append({
                    "id": f"small_{i//chunk_size_small}",
                    "text": text[i : i + chunk_size_small],
                    "source": base_id,
                    "vec": None # To be calculated
                })
            
            print(f"   ğŸ”ª Sliced: {len(self.big_chunks)} Big Chunks, {len(self.small_chunks)} Small Chunks.")
            
            # Encode Small Chunks for retrieval
            self._batch_encode_small_chunks()
            
        except Exception as e:
            print(f"âŒ File read error: {e}")

    def _batch_encode_small_chunks(self):
        """Only encode small chunks for vector search"""
        if not self.embed_model or not self.small_chunks: return
        print("   ğŸ“Š Vectorizing Small Chunks...")
        texts = [c['text'] for c in self.small_chunks]
        embeddings = self.embed_model.encode(texts, normalize_embeddings=True,show_progress_bar=False)
        for i, chunk in enumerate(self.small_chunks):
            chunk['vec'] = embeddings[i]

    # =========================================================================
    # Phase 1: Global Backbone Extraction (Qwen)
    # =========================================================================

    def _stage1_extract_backbone(self, doc_path: str, user_intent: str):
        """
        Layer 1: ä¸»å¹²æå–
        ç›®æ ‡ï¼šæå–å…¨ç¯‡çš„æ ¸å¿ƒæ¦‚å¿µå’Œæœ€å®è§‚çš„æµç¨‹/å…³ç³»ã€‚
        ç‰¹ç‚¹ï¼šchunk_id ä¸ºç©ºï¼Œå› ä¸ºæ˜¯å…¨ç¯‡æ€»ç»“ã€‚
        """
        print(f"\nğŸ—ï¸ [Layer 1] Global Backbone Extraction (Intent: {user_intent})...")
        
        system_prompt = (
            "You are a Knowledge Graph Architect responsible for the 'Skeleton' of the graph.\n"
            "Your goal is to identify the **top-level** entities and relationships that govern the document."
        )
        
        user_prompt = (
            f"User Intent: \"{user_intent}\"\n"
            "Task:\n"
            "1. Read the ENTIRE document.\n"
            "2. Extract 10-20 **Backbone Nodes**. These must be the most critical concepts (System Names, Key Modules, Core Theories).\n"
            "3. Extract **Backbone Edges** that show high-level flow or architecture.\n"
            "4. **Ignore** minor details, implementation specifics, or examples.\n\n"
            "Output JSON Schema:\n"
            "{\n"
            "  \"nodes\": [{\"id\": \"CoreConcept\", \"desc\": \"High-level definition\"}],\n"
            "  \"edges\": [{\"src\": \"NodeA\", \"dst\": \"NodeB\", \"desc\": \"Architectural relationship\"}]\n"
            "}"
        )
        
        try:
            resp = self.global_planner.chat(
                messages=[{"role": "user", "content": user_prompt}],
                system_prompt=system_prompt,
                file_path=doc_path, # Qwen Agent handles file reading
                json_mode=True
            )
            data = json.loads(clean_json_response(resp))
            
            count = self._update_graph(data, chunk_id="global_summary", node_type="backbone", weight_boost=5.0)
            print(f"   âœ… Backbone Established: {count} elements added.")
            return data.get("nodes", []) # Return list of dicts for next stage context
        except Exception as e:
            print(f"   âŒ Layer 1 Failed: {e}")
            return []

    # =========================================================================
    # Phase 2: Intermediate Structure (DeepSeek + Big Chunks)
    # =========================================================================
    def _stage2_intermediate_enrichment(self, backbone_nodes: List[Dict], user_intent: str):
        """
        [Layer 2 - Concurrent] ä¸­å±‚å¡«å…… (å¤šçº¿ç¨‹å¹¶å‘ç‰ˆ)
        """
        print(f"\nğŸŒ‰ [Layer 2] Intermediate Structure Enrichment ({len(self.big_chunks)} Big Chunks)...")
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡ (åªè¯»æ“ä½œï¼Œä¸éœ€è¦é”)
        backbone_ids = [n['id'] for n in backbone_nodes]
        backbone_context_str = ", ".join(backbone_ids[:50])
        
        # å®šä¹‰å•ä¸ª Chunk çš„å¤„ç†ä»»åŠ¡
        def process_single_chunk(chunk):
            system_prompt = (
                "You are a Structural Engineer. Your goal is to Bridge local details to the Global Backbone."
                "Prioritize connecting to existing nodes, but also establish local self-contained structures."
            )
            
            user_prompt = (
                f"User Intent: \"{user_intent}\"\n"
                f"**Global Backbone Context**: {backbone_context_str}\n\n"
                f"Current Text Fragment ({chunk['id']}):\n"
                f"```\n{chunk['text']}\n```\n\n"
                "Task:\n"
                "1. **PRIORITY 1 - Anchor to Backbone**: Identify how entities in this text relate to the 'Global Backbone Context'. Create edges connecting local entities to these Backbone Nodes.\n"
                "2. **PRIORITY 2 - Local Structure**: Extract important entities/relationships that are defined LOCALLY in this text, even if they don't directly touch the Backbone yet.\n"
                "3. **Completeness**: Do not ignore a valid relationship just because it's not in the backbone.\n\n"
                "4. **Find As More Nodes And Edges As You Can**"
                "Output JSON:\n"
                "{\n"
                "  \"nodes\": [{\"id\": \"EntityName\", \"desc\": \"Contextual definition\"}],\n"
                "  \"edges\": [{\"src\": \"Source\", \"dst\": \"Target\", \"desc\": \"Relation\"}]\n"
                "}"
            )
            
            try:
                # 1. ç½‘ç»œè¯·æ±‚
                resp = self.local_extractor.chat(
                    messages=[{"role": "user", "content": user_prompt}],
                    system_prompt=system_prompt,
                    json_mode=True
                )
                
                # === ğŸ› ï¸ [DEBUG] æ‰“å°åŸå§‹å“åº”çš„å‰50ä¸ªå­—ç¬¦ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯æ ¹æœ¬æ²¡è¿”å› JSON ===
                # print(f"   [Raw Resp Snippet] {resp[:50].replace('\n', ' ')}...") 
                
                cleaned_resp = clean_json_response(resp) # å»ºè®®æŠŠ clean æ‹¿å‡ºæ¥å•ç‹¬èµ‹å€¼ï¼Œæ–¹ä¾¿è°ƒè¯•
                data = json.loads(cleaned_resp)
                
                # 2. å›¾å†™å…¥
                with self.lock:
                    self._update_graph(data, chunk_id=chunk['id'], weight_boost=5.0)
                return True
                
            except json.JSONDecodeError as je:
                # ğŸš¨ è¿™æ˜¯æœ€å¸¸è§çš„é”™è¯¯ï¼šLLM è¿”å›çš„ä¸æ˜¯åˆæ³• JSON
                print(f"   âŒ [Stage 2 JSON Error] Chunk: {chunk['id']}")
                print(f"      -> Resp received: {resp}") # æ‰“å°å‡ºæ¥çœ‹çœ‹å®ƒåˆ°åº•å›äº†ä»€ä¹ˆé¬¼
                return False
            except Exception as e:
                # ğŸš¨ å…¶ä»–é”™è¯¯ï¼ˆç½‘ç»œè¶…æ—¶ç­‰ï¼‰
                print(f"   âŒ [Stage 2 Error] Chunk {chunk['id']}: {e}")
                return False

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        # max_workers å»ºè®®è®¾ä¸º 5-10ï¼Œå¤ªé«˜ä¼šè§¦å‘ API Rate Limit
        with ThreadPoolExecutor(max_workers=8) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [executor.submit(process_single_chunk, chunk) for chunk in self.big_chunks]
            
            # ç­‰å¾…å®Œæˆå¹¶æ˜¾ç¤ºç®€å•è¿›åº¦
            completed_count = 0
            for future in as_completed(futures):
                completed_count += 1
                if completed_count % 2 == 0:
                    print(f"   Processed {completed_count}/{len(self.big_chunks)} big chunks...", end='\r')
        
        print(f"\n   âœ… Layer 2 Complete.")

    # =========================================================================
    # Phase 3: Local Drill-down (Concurrent)
    # =========================================================================

    def _stage3_local_drilldown(self, user_intent: str):
        """
        [Layer 3 - Concurrent] ç»†èŠ‚ä¸‹é’» (å¤šçº¿ç¨‹å¹¶å‘ç‰ˆ)
        """
        print(f"\nğŸ’ [Layer 3] Semantic Local Drill-down (Concurrent)...")
        
        # 1. ç­›é€‰é«˜ä¼˜èŠ‚ç‚¹ (è¯»æ“ä½œï¼Œæ— éœ€é”)
        node_importance = nx.get_node_attributes(self.graph, 'importance')
        if not node_importance: return

        sorted_nodes = sorted(
            self.graph.nodes(), 
            key=lambda n: (node_importance.get(n, 0), self.graph.degree(n)), 
            reverse=True
        )
        focus_targets = sorted_nodes
        print(f"   ğŸ¯ Focus Targets: {focus_targets[:5]}... (Total {len(focus_targets)})")
        
        # çº¿ç¨‹å®‰å…¨çš„å»é‡é›†åˆ
        processed_chunk_ids = set()
        chunk_lock = threading.Lock() # ä¸“é—¨ä¿æŠ¤ processed_chunk_ids çš„å°é”

        # å®šä¹‰å•ä¸ª Focus Node çš„å¤„ç†ä»»åŠ¡
        def process_single_focus_node(focus_node_id):
            # è·å–èŠ‚ç‚¹æè¿° (è¯»å›¾ï¼Œå»ºè®®åŠ  try-except é˜²æ­¢åˆ«çš„çº¿ç¨‹åˆ äº†èŠ‚ç‚¹)
            try:
                node_desc = self.graph.nodes[focus_node_id].get('description', '')
            except KeyError:
                return 0
                
            rich_query = f"{focus_node_id}: {node_desc}"
            
            # æ£€ç´¢ (åªè¯»ï¼Œå¹¶å‘å®‰å…¨)
            hits = self._search_small_chunks(query=rich_query, top_k=50)
            
            tasks_run = 0
            for chunk in hits:
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç† (åŠ é”æ£€æŸ¥)
                with chunk_lock:
                    if chunk['id'] in processed_chunk_ids:
                        continue
                    processed_chunk_ids.add(chunk['id'])
                
                # æ‰§è¡Œæå–
                self._extract_constrained_details_concurrent(chunk, focus_node_id, node_desc, user_intent)
                tasks_run += 1
            return tasks_run

        # å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(process_single_focus_node, target) for target in focus_targets]
            
            # ç­‰å¾…ç»“æœ
            total_chunks_analyzed = 0
            for future in as_completed(futures):
                total_chunks_analyzed += future.result()
                print(f"   Drilling down... ({total_chunks_analyzed} chunks analyzed)", end='\r')

        print(f"\n   âœ… Layer 3 Complete. Analyzed {len(processed_chunk_ids)} unique small chunks.")

    def _extract_constrained_details_concurrent(self, chunk: Dict, focus_node: str, focus_desc: str, user_intent: str):
        """
        å¹¶å‘ç‰ˆçš„ Layer 3 æå–å™¨
        åŒºåˆ«ï¼šå†…éƒ¨ä½¿ç”¨äº† self.lock æ¥ä¿æŠ¤ _update_graph
        """
        system_prompt = (
            "You are a Detail Analyst. Your primary mission is to expand the graph around the Focus Node."
            "Simultaneously, capture other high-value dense relationships in the text."
        )
        
        user_prompt = (
            f"User Intent: \"{user_intent}\"\n"
            f"**Primary Focus Node**: '{focus_node}' (Context: {focus_desc})\n\n"
            f"Text Fragment ({chunk['id']}):\n"
            f"```\n{chunk['text']}\n```\n\n"
            "Task:\n"
            f"1. **Core Task**: Extract every possible relationship involving **'{focus_node}'**. Explain HOW it interacts with others.\n"
            f"2. **Secondary Task**: If you identify other clear, high-value relationships between entities in this text (even if '{focus_node}' is not involved), extract them as well to densify the graph.\n"
            f"3. **Constraint**: Do not hallucinate connections. If '{focus_node}' is not mentioned or implied, focus on what IS present.\n\n"
            "4. **Find As More Nodes And Edges As You Can**"
            "Output JSON:\n"
            "{\n"
            "  \"nodes\": [{\"id\": \"Entity\", \"desc\": \"Definition\"}],\n"
            "  \"edges\": [{\"src\": \"Source\", \"dst\": \"Target\", \"desc\": \"Specific relation\"}]\n"
            "}"
        )
        
        try:
            # LLM æ¨ç†
            resp = self.local_extractor.chat(
                messages=[{"role": "user", "content": user_prompt}],
                system_prompt=system_prompt,
                json_mode=True
            )
            
            # === ğŸ› ï¸ [DEBUG] æ£€æŸ¥å“åº” ===
            if not resp:
                print(f"   âš ï¸ [Stage 3] Empty response for focus: {focus_node}")
                return

            cleaned_resp = clean_json_response(resp)
            data = json.loads(cleaned_resp)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œä¹Ÿæ‰“å°ä¸€ä¸‹
            if not data.get("nodes") and not data.get("edges"):
                 # è¿™è¯´æ˜ LLM è§‰å¾—è¿™æ®µè¯è·Ÿ Focus Node æ²¡å…³ç³»ï¼Œæˆ–è€…æ˜¯ Prompt é™åˆ¶å¤ªæ­»äº†
                # print(f"   â„¹ï¸ [Stage 3] No relations found for {focus_node} in chunk {chunk['id']}")
                pass 
            
            # å›¾æ›´æ–°
            with self.lock:
                self._update_graph(data, chunk_id=chunk['id'], weight_boost=1.0)
                
        except json.JSONDecodeError:
            # ğŸš¨ å¿…é¡»æŠŠè¿™ä¸ªæ‰“å°å‡ºæ¥ï¼ŒDeepSeek ç»å¸¸åœ¨ JSON åé¢åŠ åºŸè¯å¯¼è‡´è§£æå¤±è´¥
            print(f"   âŒ [Stage 3 JSON Fail] Focus: {focus_node} | Chunk: {chunk['id']}")
            print(f"      -> Content: {resp}") 
        except Exception as e:
            # ğŸš¨ ä¹‹å‰è¿™é‡Œæ˜¯ passï¼Œç°åœ¨å¿…é¡»çœ‹æŠ¥é”™
            print(f"   âŒ [Stage 3 Exception] {e}")
    # =========================================================================
    # Common Graph Updater
    # =========================================================================

    def _update_graph(self, data: Dict, chunk_id: str, weight_boost: float, node_type:str = "derived") -> int:
        """
        ç»Ÿä¸€çš„å›¾æ›´æ–°å…¥å£ã€‚
        è´Ÿè´£ï¼š
        1. èŠ‚ç‚¹/è¾¹çš„å»é‡ä¸åˆå¹¶ã€‚
        2. Provenance (source_chunks) çš„æ›´æ–°ã€‚
        3. æƒé‡çš„ç´¯åŠ ã€‚
        """
        # === ğŸ› ï¸ [DEBUG START] ===
        new_nodes = len(data.get("nodes", []))
        new_edges = len(data.get("edges", []))
        if new_nodes > 0 or new_edges > 0:
            print(f"   [Graph Update] Source: {chunk_id} | Type: {node_type} | +Nodes: {new_nodes} | +Edges: {new_edges}")
        else:
            print(f"   âš ï¸ [Graph Update] Source: {chunk_id} returned EMPTY data!")
        # === ğŸ› ï¸ [DEBUG END] ===
        
        count = 0
        # ... åŸæœ‰ä»£ç ç»§ç»­ ...
        count = 0
        
        # 1. Update Nodes
        for n_data in data.get("nodes", []):
            nid = n_data.get('id')
            if not nid: continue
            
            count += 1
            if not self.graph.has_node(nid):
                self.graph.add_node(nid, 
                                    description=n_data.get('desc', ''), 
                                    type=node_type,
                                    source_chunks={chunk_id} if chunk_id else set(),
                                    importance=weight_boost)
            else:
                # Merge logic
                node = self.graph.nodes[nid]
                if chunk_id:
                    node['source_chunks'].add(chunk_id)
                node['importance'] += weight_boost
                
                # å¦‚æœç°æœ‰æè¿°å¤ªçŸ­ï¼Œä¸”æ–°æè¿°è¾ƒé•¿ï¼Œæ›´æ–°æè¿°
                if len(n_data.get('desc', '')) > len(node.get('description', '')):
                    node['description'] = n_data['desc']

        # 2. Update Edges
        for e_data in data.get("edges", []):
            src, dst = e_data.get('src'), e_data.get('dst')
            if not src or not dst: continue
            
            # Ensure endpoints exist (Auto-create if missing to avoid errors)
            for pt in [src, dst]:
                if not self.graph.has_node(pt):
                    self.graph.add_node(pt, description="Inferred", type="inferred", source_chunks={chunk_id}, importance=1.0)
            
            # Add/Merge Edge
            desc = e_data.get('desc', 'related')
            weight = weight_boost
            
            if self.graph.has_edge(src, dst):
                # å¦‚æœè¾¹å·²å­˜åœ¨ï¼Œæˆ‘ä»¬å°†æ–°æè¿°è¿½åŠ è¿›å»ï¼Œå½¢æˆä¸°å¯Œçš„ä¸Šä¸‹æ–‡
                old_data = self.graph.edges[src, dst]
                if desc not in old_data['description']:
                    old_data['description'] += f" | {desc}"
                old_data['weight'] += weight
                # è®°å½• chunk_id (è¿™é‡Œç®€å•è¦†ç›–ï¼Œæˆ–è€…æ‰©å±•ä¸ºåˆ—è¡¨)
                if chunk_id:
                    old_data['source_chunk_id'] = chunk_id 
            else:
                self.graph.add_edge(src, dst, 
                                    description=desc, 
                                    weight=weight, 
                                    source_chunk_id=chunk_id)
        self.graph_version += 1
        return count
    
    # =========================================================================
    # Phase 4: Graph Optimization (Backbone-Centric Rewiring)
    # =========================================================================

    def _stage4_graph_optimization(self, max_iterations: int = 3):
        """
        Stage 4: å›¾è°±ç»“æ„ä¼˜åŒ–
        åŸºäºè¿é€šåˆ†é‡åˆ†æï¼Œæ¸…æ´—å™ªéŸ³ï¼Œåˆå¹¶åŒä¹‰è¯ï¼Œå¼ºåˆ¶è¿é€šå­¤å²›ã€‚
        """
        print(f"\nâš¡ [Stage 4] Graph Optimization (Backbone-Centric Rewiring)...")
        
        for i in range(max_iterations):
            # 1. æå–å¼±è¿é€šåˆ†é‡ (é’ˆå¯¹ DiGraph)
            # å¼±è¿é€šæ„å‘³ç€æŠŠè¾¹çœ‹ä½œæ— å‘æ—¶æ˜¯è¿é€šçš„ï¼Œè¿™ç¬¦åˆæˆ‘ä»¬å¯¹â€œå­¤å²›â€çš„å®šä¹‰
            components = list(nx.weakly_connected_components(self.graph))
            
            if len(components) <= 1:
                print("   âœ… Graph is fully connected. Optimization finished.")
                break
            
            # æŒ‰èŠ‚ç‚¹æ•°é‡æ’åºï¼Œæœ€å¤§çš„ä¸ºä¸»å¹²
            components.sort(key=len, reverse=True)
            backbone_nodes = components[0]
            fragment_nodes = set().union(*components[1:])
            
            # å¦‚æœä¸»å¹²å¤ªå°ï¼ˆæ¯”å¦‚åˆšå¼€å§‹æ„å»ºï¼‰ï¼Œå¯èƒ½ä¸éœ€è¦ä¼˜åŒ–ï¼Œæˆ–è€…é€»è¾‘ä¸åŒ
            if len(backbone_nodes) < 3:
                print("   âš ï¸ Graph too small to optimize.")
                break

            print(f"   ğŸ”„ [Iter {i+1}] Backbone: {len(backbone_nodes)} nodes | Fragments: {len(components)-1} clusters | Orphan Nodes: {len(fragment_nodes)}")

            # 2. å‡†å¤‡ä¸Šä¸‹æ–‡ (Backbone Context)
            # é‡‡æ ·ä¸€äº› Backbone çš„æ ¸å¿ƒè¾¹ï¼Œè®© LLM çŸ¥é“ä¸»å¹²é‡Œæœ‰ä»€ä¹ˆ
            backbone_subgraph = self.graph.subgraph(backbone_nodes)
            # ä¼˜å…ˆé€‰æ‹© importance é«˜çš„èŠ‚ç‚¹çš„è¾¹
            sorted_edges = sorted(backbone_subgraph.edges(data=True), 
                                  key=lambda x: self.graph.nodes[x[0]].get('importance', 0) + self.graph.nodes[x[1]].get('importance', 0), 
                                  reverse=True)
            
            backbone_desc_lines = []
            for u, v, d in sorted_edges[:100]: # é™åˆ¶ Tokenï¼Œåªç»™ Top 100 è¾¹
                u_desc = self.graph.nodes[u].get('description', '')[:50]
                v_desc = self.graph.nodes[v].get('description', '')[:50]
                rel = d.get('description', 'related')[:30]
                backbone_desc_lines.append(f"({u}) --[{rel}]--> ({v})")
            
            backbone_str = "\n".join(backbone_desc_lines)

            # 3. å‡†å¤‡ç›®æ ‡æ•°æ® (Fragment Context)
            # å¯¹äºå­¤å²›ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬çš„å†…å®¹å‘ç»™ LLM
            fragment_subgraph = self.graph.subgraph(fragment_nodes)
            fragment_lines = []
            
            # æå–ç¢ç‰‡ä¸­çš„è¾¹
            for u, v, d in list(fragment_subgraph.edges(data=True))[:80]:
                u_desc = self.graph.nodes[u].get('description', 'No desc')
                v_desc = self.graph.nodes[v].get('description', 'No desc')
                fragment_lines.append(f"EDGE: ({u} [desc: {u_desc}]) --[{d.get('description','?')}]--> ({v})")
            
            # æå–ç¢ç‰‡ä¸­çš„å­¤ç«‹ç‚¹ (æ²¡æœ‰è¾¹çš„ç‚¹)
            isolates = [n for n in fragment_nodes if fragment_subgraph.degree(n) == 0]
            for node in isolates[:30]:
                desc = self.graph.nodes[node].get('description', 'No desc')
                fragment_lines.append(f"NODE: {node} [desc: {desc}]")
                
            fragment_str = "\n".join(fragment_lines)
            
            if not fragment_str.strip():
                print("   -> No meaningful fragments found. Cleaning leftovers.")
                self.graph.remove_nodes_from(list(nx.isolates(self.graph)))
                continue

            # 4. LLM å†³ç­–
            self._execute_optimization_prompt(backbone_str, fragment_str, backbone_nodes)
            self.graph_version += 1

        # æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤ä»ç„¶æ— æ³•è¿æ¥çš„å¾®å°å­¤ç«‹ç‚¹
        final_isolates = list(nx.isolates(self.graph))
        if final_isolates:
            self.graph.remove_nodes_from(final_isolates)
            print(f"   ğŸ§¹ Final Cleanup: Removed {len(final_isolates)} stubborn isolated nodes.")

    def _execute_optimization_prompt(self, backbone_str, fragment_str, backbone_nodes):
        """æ‰§è¡Œä¼˜åŒ–æŒ‡ä»¤å¹¶åº”ç”¨ä¿®æ”¹"""
        system_prompt = "You are a Knowledge Graph Cleaner & Linker."
        
        user_prompt = (
            f"=== MAIN KNOWLEDGE BACKBONE (READ ONLY) ===\n"
            f"These nodes are the core truth. DO NOT DELETE THEM.\n"
            f"{backbone_str}\n\n"
            f"=== DISCONNECTED FRAGMENTS (TARGETS) ===\n"
            f"These entities are currently disconnected from the Backbone.\n"
            f"{fragment_str}\n\n"
            f"=== TASK ===\n"
            "Analyze the Fragments and decide their fate:\n"
            "1. **DELETE**: If it is noise, generic headers (e.g. 'Table 1'), or irrelevant.\n"
            "2. **MERGE**: If a fragment entity is a SYNONYM of a Backbone entity. (e.g., 'LLMs' -> 'Large Language Models').\n"
            "   * 'source' must be Fragment Node, 'target' must be Backbone Node.\n"
            "3. **CONNECT**: If the fragment is valid but missing a link. Create a specific relationship to a Backbone Node.\n\n"
            "=== OUTPUT JSON ===\n"
            "{\n"
            "  \"operations\": [\n"
            "    {\"type\": \"DELETE\", \"nodes\": [\"bad_node_1\"]},\n"
            "    {\"type\": \"MERGE\", \"source\": \"fragment_node\", \"target\": \"backbone_node\"},\n"
            "    {\"type\": \"CONNECT\", \"source\": \"fragment_node\", \"target\": \"backbone_node\", \"desc\": \"connection logic\", \"weight\": 3}\n"
            "  ]\n"
            "}"
        )
        
        try:
            resp = self.local_extractor.chat(
                messages=[{"role": "user", "content": user_prompt}],
                system_prompt=system_prompt,
                json_mode=True
            )
            data = json.loads(clean_json_response(resp))
            ops = data.get("operations", [])
            
            if not ops: 
                print("   -> LLM suggests no changes.")
                return

            print(f"   -> Executing {len(ops)} operations...")
            
            for op in ops:
                op_type = op.get("type", "").upper()
                
                if op_type == "DELETE":
                    for n in op.get("nodes", []):
                        if n in backbone_nodes: continue # ä¿æŠ¤ä¸»å¹²
                        if self.graph.has_node(n):
                            self.graph.remove_node(n)
                            
                elif op_type == "MERGE":
                    src = op.get("source")
                    tgt = op.get("target")
                    self._merge_nodes(src, tgt, backbone_nodes)
                            
                elif op_type == "CONNECT":
                    src = op.get("source")
                    tgt = op.get("target")
                    desc = op.get("desc", "Connected by optimizer")
                    weight = op.get("weight", 2.0)
                    if self.graph.has_node(src) and self.graph.has_node(tgt):
                        self.graph.add_edge(src, tgt, description=desc, weight=weight, source_chunk_id="optimization")

        except Exception as e:
            print(f"   âš ï¸ Optimization step failed: {e}")

    def _merge_nodes(self, src, tgt, backbone_nodes):
        """
        å®‰å…¨çš„èŠ‚ç‚¹åˆå¹¶é€»è¾‘ï¼šSrc -> Tgt
        1. è½¬ç§»è¾¹
        2. åˆå¹¶å…ƒæ•°æ® (source_chunks, importance)
        3. åˆ é™¤ Src
        """
        if not (self.graph.has_node(src) and self.graph.has_node(tgt)): return
        
        # é˜²æ­¢åå‘åˆå¹¶ï¼ˆæŠŠä¸»å¹²åˆåˆ°äº†ç¢ç‰‡é‡Œï¼‰
        if src in backbone_nodes and tgt not in backbone_nodes:
            # Swap logic to protect backbone
            src, tgt = tgt, src
            
        # 1. Merge Attributes
        tgt_node = self.graph.nodes[tgt]
        src_node = self.graph.nodes[src]
        
        # åˆå¹¶ chunks
        tgt_node['source_chunks'].update(src_node.get('source_chunks', set()))
        # ç´¯åŠ  importance
        tgt_node['importance'] = tgt_node.get('importance', 1.0) + src_node.get('importance', 1.0)
        # æè¿°å–æœ€é•¿çš„
        if len(src_node.get('description', '')) > len(tgt_node.get('description', '')):
            tgt_node['description'] = src_node['description']

        # 2. Transfer Edges
        # Out edges: src -> nbr  ==>  tgt -> nbr
        for _, nbr, data in list(self.graph.out_edges(src, data=True)):
            if nbr == tgt: continue # é¿å…è‡ªç¯
            if self.graph.has_edge(tgt, nbr):
                # è¾¹å·²å­˜åœ¨ï¼Œåˆå¹¶æƒé‡
                self.graph[tgt][nbr]['weight'] += data.get('weight', 1.0)
            else:
                self.graph.add_edge(tgt, nbr, **data)
        
        # In edges: nbr -> src  ==>  nbr -> tgt
        for nbr, _, data in list(self.graph.in_edges(src, data=True)):
            if nbr == tgt: continue
            if self.graph.has_edge(nbr, tgt):
                self.graph[nbr][tgt]['weight'] += data.get('weight', 1.0)
            else:
                self.graph.add_edge(nbr, tgt, **data)
                
        # 3. Remove Source
        self.graph.remove_node(src)
        # print(f"      [Merged] {src} -> {tgt}")

    # =========================================================================
    # Main Build Entry
    # =========================================================================

    def build_graph(self, doc_path: str, user_intent: str = "General Analysis"):
        print(f"ğŸš€ Starting Build Process for {doc_path}...")
        
        # Step 0: Slice
        self._chunk_document_dual_layer(doc_path)
        if not self.big_chunks: return
        
        # Step 1: Layer 1 - Global Backbone
        backbone_nodes = self._stage1_extract_backbone(doc_path, user_intent)
        self.graph_version += 1
        # Step 2: Layer 2 - Intermediate
        self._stage2_intermediate_enrichment(backbone_nodes, user_intent)
        self.graph_version += 1
        # Step 3: Layer 3 - Local Drill-down
        self._stage3_local_drilldown(user_intent)
        self.graph_version +=1
        # Step 4: Graph Optimization ---
        self._stage4_graph_optimization(max_iterations=3)
        self.graph_version+=1
        # Save
        self.save_graph()
        print(f"\nâœ… Graph Build Complete.")
        print(f"   Nodes: {self.graph.number_of_nodes()}")
        print(f"   Edges: {self.graph.number_of_edges()}")

    # =========================================================================
    # Graph-First Retrieval Engine
    # =========================================================================

    def _get_chunk_text_by_id(self, chunk_id: str) -> Optional[str]:
        """
        [Helper] æ ¹æ® ID ä»å†…å­˜åˆ—è¡¨ä¸­æŸ¥æ‰¾åŸå§‹æ–‡æœ¬ã€‚
        ä¼˜å…ˆæŸ¥æ‰¾ Small Chunks (ç»†èŠ‚)ï¼Œå…¶æ¬¡ Big Chunks (èƒŒæ™¯)ã€‚
        """
        if not chunk_id: return None
        
        # 1. Try Small Chunks (Priority)
        for c in self.small_chunks:
            if c['id'] == chunk_id:
                return c['text']
        
        # 2. Try Big Chunks (Fallback)
        for c in self.big_chunks:
            if c['id'] == chunk_id:
                return c['text']
                
        return None

    def search(self, query: str, top_k: int = 3) -> str:
        """
        [Drill-Down Optimized Search Engine]
        
        æ‰§è¡Œé€»è¾‘:
        1. **Semantic Anchor**: Query <-> (Node + Description). å¯»æ‰¾è¯­ä¹‰æœ€æ¥è¿‘çš„æ¦‚å¿µé”šç‚¹ã€‚
        2. **Graph Expansion**: æ‰©æ•£ 1-Hopï¼Œè·å–å…³ç³»æè¿°ã€‚
        3. **Weighted Voting**: 
           - æŠ•ç¥¨ç»™ Chunkã€‚
           - Small Chunk æƒé‡ > Big Chunkã€‚
           - åŒ…å« Anchor çš„ Chunk æƒé‡å€å¢ã€‚
        4. **Rich Context Assembly**: 
           - ç»„è£…å®šä¹‰(Definitions)ã€å…³ç³»(Relations)ã€è¯æ®(Evidence)ã€‚
        """
        if self.graph.number_of_nodes() == 0: 
            return "Knowledge Graph is empty."
        
        print(f"\nğŸ” [Drill-Down] Searching Graph for: \"{query}\"")
        query_vec = self._get_embedding(query)
        
        # =================================================
        # Step 1: Anchor Identification (Pure Semantic)
        # =================================================
        node_candidates = []
        for n, attr in self.graph.nodes(data=True):
            # ç»„åˆ "ID + Description" ä»¥æ•æ‰ç²¾å‡†è¯­ä¹‰
            desc = attr.get('description', 'No description')
            node_rich_text = f"{n}: {desc}"
            
            n_vec = self._get_embedding(node_rich_text)
            score = self._cosine_similarity(query_vec, n_vec)
            
            # è¯­ä¹‰è¿‡æ»¤ (Threshold 0.35)
            if score > 0.35: 
                node_candidates.append((score, n, desc))
        
        # æ’åºå¹¶æˆªå– Top 5 é”šç‚¹
        node_candidates.sort(key=lambda x: x[0], reverse=True)
        top_anchors = node_candidates[:5] # List of (score, node_id, desc)
        anchor_ids = [n for s, n, d in top_anchors]
        
        print(f"   âš“ Top Anchors: {anchor_ids}")
        
        if not anchor_ids: 
            return "No relevant concepts found in the Knowledge Graph."

        # =================================================
        # Step 2: Subgraph Expansion (Contextualization)
        # =================================================
        subgraph_nodes = set(anchor_ids)
        edge_context = []
        
        for anchor in anchor_ids:
            # Outgoing Edges
            for nbr in self.graph.successors(anchor):
                if nbr not in subgraph_nodes:
                    subgraph_nodes.add(nbr)
                    attr = self.graph.edges[anchor, nbr]
                    edge_context.append(f"â€¢ {anchor} -> {nbr}: {attr.get('description','')}")
            
            # Incoming Edges (æº¯æº)
            for nbr in self.graph.predecessors(anchor):
                if nbr not in subgraph_nodes:
                    subgraph_nodes.add(nbr)
                    attr = self.graph.edges[nbr, anchor]
                    edge_context.append(f"â€¢ {nbr} -> {anchor}: {attr.get('description','')}")

        # =================================================
        # Step 3: Weighted Voting (Detail First)
        # =================================================
        chunk_votes = Counter()
        chunk_to_entities = defaultdict(list) # è®°å½•æ¯ä¸ª Chunk å‘½ä¸­äº†å“ªäº›å›¾è°±èŠ‚ç‚¹
        
        for node in subgraph_nodes:
            s_chunks = self.graph.nodes[node].get('source_chunks', set())
            
            for cid in s_chunks:
                if not cid or cid == "global_summary": continue
                
                # è®°å½•å®ä½“å‘½ä¸­æƒ…å†µï¼Œç”¨äºæœ€åå±•ç¤º
                chunk_to_entities[cid].append(node)
                
                # æ‰“åˆ†é€»è¾‘
                score = 1.0
                # A. Anchor Bonus: åŒ…å«æ ¸å¿ƒé”šç‚¹
                if node in anchor_ids:
                    score += 2.0
                
                # B. Granularity Bonus: å°åˆ‡ç‰‡ä¼˜å…ˆ
                if cid.startswith("small_"):
                    score += 1.5 
                elif cid.startswith("big_"):
                    score += 0.5 
                
                chunk_votes[cid] += score

        top_chunk_ids = [cid for cid, v in chunk_votes.most_common(top_k)]
        print(f"   ğŸ—³ï¸  Top Chunks (Weighted): {top_chunk_ids}")

        # =================================================
        # Step 4: Rich Context Assembly
        # =================================================
        final_context = []
        
        # --- Section 1: Core Concept Definitions (å¯¹é½æœ¯è¯­) ---
        final_context.append("### ğŸ§  Core Concepts & Definitions")
        for score, n, desc in top_anchors:
            final_context.append(f"- **{n}**: {desc} (Confidence: {score:.2f})")
            
        # --- Section 2: Knowledge Graph Logic (æ¨ç†è·¯å¾„) ---
        final_context.append("\n### ğŸ•¸ï¸ Graph Logic Pathways")
        if edge_context:
            # æ’åºï¼šä¼˜å…ˆå±•ç¤ºæè¿°æ›´é•¿ã€æ›´è¯¦ç»†çš„è¾¹
            sorted_edges = sorted(list(set(edge_context)), key=lambda x: len(x), reverse=True)
            final_context.extend(sorted_edges[:15]) # å±•ç¤º Top 15 æ¡è¾¹
        else:
            final_context.append("(No explicit relationships found in subgraph)")

        # --- Section 3: Source Evidence (åŸå§‹ç‰‡æ®µ) ---
        final_context.append("\n### ğŸ“– Detailed Source Evidence")
        for cid in top_chunk_ids:
            text = self._get_chunk_text_by_id(cid)
            if not text: continue
            
            # è·å–è¯¥ Chunk å‘½ä¸­çš„å›¾è°±å®ä½“ï¼Œè¾…åŠ© LLM ç†è§£è¿™æ®µè¯çš„é‡ç‚¹
            hit_nodes = chunk_to_entities.get(cid, [])
            hit_anchors = [n for n in hit_nodes if n in anchor_ids]
            other_hits = [n for n in hit_nodes if n not in anchor_ids][:5] # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            
            header_info = f"**[Source ID: {cid}]**"
            if hit_anchors:
                header_info += f"\n*Key Anchors Hit*: {', '.join(hit_anchors)}"
            if other_hits:
                header_info += f"\n*Related Entities*: {', '.join(other_hits)}..."
                
            final_context.append(f"\n{header_info}")
            final_context.append(f"```text\n{text}\n```")
            final_context.append("---")
            
        return "\n".join(final_context)
    
    # =========================================================================
    # Helpers
    # =========================================================================

    def _search_small_chunks(self, query: str, top_k: int = 3) -> List[Dict]:
        """Utility for Layer 3"""
        if not self.small_chunks: return []
        q_vec = self._get_embedding(query)
        scores = []
        for c in self.small_chunks:
            if c['vec'] is None: continue
            s = self._cosine_similarity(q_vec, c['vec'])
            scores.append((s, c))
        scores.sort(key=lambda x: x[0], reverse=True)
        return [c for s, c in scores[:top_k]]

    def _get_embedding(self, text: str) -> np.ndarray:
        if not self.embed_model: return np.zeros(1024)
        return self.embed_model.encode(text, normalize_embeddings=True,show_progress_bar=False)

    def _cosine_similarity(self, v1, v2):
        return float(np.dot(v1, v2))

    def clear_graph(self):
        """
        [System] é‡ç½®å†…å­˜çŠ¶æ€
        ç”¨äº reload_db æ—¶å½»åº•æ¸…é™¤æ—§æ•°æ®ï¼Œæˆ–è€…åœ¨é‡æ–° build_graph å‰æ¸…ç©ºå½“å‰çŠ¶æ€ã€‚
        """
        # 1. é‡ç½®å›¾ç»“æ„
        self.graph = nx.DiGraph()
        
        # 2. é‡ç½®åˆ‡ç‰‡åˆ—è¡¨
        self.small_chunks = []
        self.big_chunks = []
        
        # 3. (å¯é€‰) å¦‚æœä½ æœ‰ç¼“å­˜æœºåˆ¶ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ¸…ç†
        
        print("ğŸ§¹ [System] Memory cleared (Graph & Chunks reset).")
    # =========================================================================
    # Persistence (JSON Edition - Compatible with your old workflow)
    # =========================================================================

    def save_graph(self):
        """
        ä¿å­˜å›¾è°±åˆ° JSON (ä¸ºäº†å…¼å®¹æ€§å’Œå¯è¯»æ€§)
        """
        try:
            # 1. å‡†å¤‡å›¾æ•°æ®
            # NetworkX çš„ node_link_data å¯ä»¥æŠŠå›¾è½¬ä¸º JSON å‹å¥½çš„å­—å…¸
            # ä½†æˆ‘ä»¬éœ€è¦å…ˆå¤„ç† Set ç±»å‹çš„å±æ€§ï¼Œå› ä¸º JSON ä¸æ”¯æŒ Set
            G_export = self.graph.copy()
            for n in G_export.nodes():
                # Set -> List
                s = G_export.nodes[n].get('source_chunks', set())
                G_export.nodes[n]['source_chunks'] = list(s)
            
            # è½¬æ¢ä¸ºå­—å…¸ç»“æ„
            graph_json_data = nx.node_link_data(G_export)
            
            # 2. å†™å…¥ graph.json
            with open(os.path.join(self.persist_dir, "graph.json"), "w", encoding='utf-8') as f:
                json.dump(graph_json_data, f, ensure_ascii=False, indent=2)
            
            # 3. å†™å…¥ chunks.pkl (å‘é‡æ•°æ®è¿˜æ˜¯æ¨è pickleï¼Œå›  numpy array è½¬ json è¾ƒéº»çƒ¦)
            with open(os.path.join(self.persist_dir, "chunks.pkl"), "wb") as f:
                pickle.dump({"small": self.small_chunks, "big": self.big_chunks}, f)
                
            print(f"ğŸ’¾ Graph saved to {self.persist_dir}/graph.json")
        except Exception as e:
            print(f"âŒ Save failed: {e}")

    def load_graph(self):
        """
        ä» JSON åŠ è½½å›¾è°±
        """
        try:
            graph_path = os.path.join(self.persist_dir, "graph.json")
            chunk_path = os.path.join(self.persist_dir, "chunks.pkl")
            
            # 1. åŠ è½½å›¾ç»“æ„
            if os.path.exists(graph_path):
                with open(graph_path, 'r', encoding='utf-8') as f:
                    graph_json_data = json.load(f)
                
                # æ¢å¤å›¾å¯¹è±¡ (Directed Graph)
                self.graph = nx.node_link_graph(graph_json_data, directed=True)
                
                # æ¢å¤æ•°æ®ç±»å‹ (List -> Set, String -> Float)
                for n in self.graph.nodes():
                    # æ¢å¤ source_chunks ä¸º Set
                    val = self.graph.nodes[n].get('source_chunks', [])
                    self.graph.nodes[n]['source_chunks'] = set(val) if isinstance(val, list) else set()
                        
                    # æ¢å¤ importance ä¸º float
                    imp = self.graph.nodes[n].get('importance', 1.0)
                    self.graph.nodes[n]['importance'] = float(imp)
            else:
                print("â„¹ï¸ No existing graph found, initializing new.")
                self.graph = nx.DiGraph()

            # 2. åŠ è½½åˆ‡ç‰‡æ•°æ®
            if os.path.exists(chunk_path):
                with open(chunk_path, "rb") as f:
                    data = pickle.load(f)
                    self.small_chunks = data.get("small", [])
                    self.big_chunks = data.get("big", [])
                    
            print(f"ğŸ“‚ Loaded: {self.graph.number_of_nodes()} nodes from JSON.")
            
        except Exception as e:
            print(f"âš ï¸ Load warning (Starting fresh): {e}")
            self.graph = nx.DiGraph()
            self.small_chunks = []
            self.big_chunks = []

    def reload_db(self, new_persist_dir: str):
        """
        [System] å®‰å…¨åˆ‡æ¢é¡¹ç›®æ•°æ®åº“ (Auto-Save & Switch)
        
        åŠŸèƒ½:
        1. è‡ªåŠ¨ä¿å­˜: åˆ‡æ¢å‰å¼ºåˆ¶ä¿å­˜å½“å‰é¡¹ç›®æ•°æ®åˆ°æ—§ç›®å½•ã€‚
        2. ç¯å¢ƒé‡ç½®: æ¸…ç©ºå†…å­˜ï¼Œé˜²æ­¢æ—§é¡¹ç›®æ•°æ®æ··å…¥æ–°é¡¹ç›®ã€‚
        3. è·¯å¾„åˆ‡æ¢: æŒ‡å‘æ–°ç›®å½•å¹¶åŠ è½½æ•°æ® (æ”¯æŒ JSON/Pickle)ã€‚
        """
        print(f"ğŸ”„ [GraphRAG] Requesting project switch to: {new_persist_dir}")
        
        # 1. [Auto-Save] è‡ªåŠ¨ä¿å­˜å½“å‰è¿›åº¦
        # åªæœ‰å½“å†…å­˜é‡Œç¡®å®æœ‰æ•°æ®æ—¶æ‰ä¿å­˜ï¼Œé¿å…ç©ºè·‘
        if self.graph.number_of_nodes() > 0 or len(self.small_chunks) > 0:
            print(f"ğŸ’¾ [Auto-Save] Saving current workspace to: {self.persist_dir} ...")
            self.save_graph() # è°ƒç”¨åˆšæ‰ä¿®æ”¹è¿‡çš„ JSON ç‰ˆ save_graph
        else:
            print("â„¹ï¸ [Info] Current workspace is empty, skipping auto-save.")

        # 2. [Switch Dir] åˆ‡æ¢è·¯å¾„å˜é‡
        self.persist_dir = new_persist_dir
        # ç¡®ä¿æ–°ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs(self.persist_dir, exist_ok=True)

        # 3. [Clear RAM] å½»åº•æ¸…ç©ºå†…å­˜å¯¹è±¡
        # è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼å¦åˆ™ä¸Šä¸€ä¸ªé¡¹ç›®çš„èŠ‚ç‚¹ä¼šæ®‹ç•™åœ¨ self.graph é‡Œ
        self.clear_graph() 
        
        # 4. [Load New] åŠ è½½æ–°é¡¹ç›®æ•°æ®
        # å°è¯•è¯»å–æ–°ç›®å½•ä¸‹çš„ graph.json å’Œ chunks.pkl
        # å¦‚æœæ˜¯æ–°ç›®å½•ï¼ˆæ— æ–‡ä»¶ï¼‰ï¼Œload_graph ä¼šè‡ªåŠ¨åˆå§‹åŒ–ç©ºçŠ¶æ€
        self.load_graph()
        
        print(f"âœ… [Switch Complete] Now working on: {new_persist_dir}")
        print(f"   Current State: {self.graph.number_of_nodes()} Nodes, {len(self.small_chunks)} Small Chunks.")

    def get_graph_snapshot(self):
        """è·å–å›¾è°±å¿«ç…§ (å¸¦ Debug æ‰“å°)"""
        try:

            current_ver = self.graph_version
            node_count = self.graph.number_of_nodes()
            edge_count = self.graph.number_of_edges()
            
            nodes = []
            for n, attr in self.graph.nodes(data=True):
                degree = self.graph.degree(n)
                size = 5 + (degree * 0.5) if degree else 5
                nodes.append({
                    "id": str(n),
                    "label": str(n),
                    "color": "#4F8BF9",
                    "val": size,
                    "title": attr.get("description", "") 
                })
            
            links = []
            for u, v, data in self.graph.edges(data=True):
                links.append({
                    "source": str(u), 
                    "target": str(v), 
                    "label": data.get("description", "") # ä¿®å¤å­—æ®µå
                })

            # ğŸ› ï¸ [DEBUG] æ‰“å°ä¸€ä¸‹ï¼Œçœ‹çœ‹å‰ç«¯åˆ°åº•æœ‰æ²¡æœ‰æ¥æ‹¿æ•°æ®
            print(f"ğŸ“¡ [Snapshot] Frontend requested. Ver: {current_ver} | Nodes: {node_count}")

            return {
                "version": current_ver, 
                "nodes": nodes, 
                "links": links
            }
        except Exception as e:
            print(f"âŒ [Snapshot Error] {e}")
            return {"version": 0, "nodes": [], "links": []}