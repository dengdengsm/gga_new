import json
import os
import shutil
import uvicorn
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from pydantic import BaseModel
from typing import Optional, List, Dict, Any, Callable
import glob
import re
from datetime import datetime
import time
import uuid
import logging

# å…³é—­ httpx (OpenAI/DeepSeek åº•å±‚é€šè®¯åº“) çš„ INFO æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)

# å¦‚æœè¿˜æœ‰å…¶ä»–å¹²æ‰°ï¼Œå¯ä»¥å°è¯•å…³é—­è¿™äº›å¸¸è§åº“çš„æ—¥å¿—
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
# --- å¼•å…¥ä½ çš„æ ¸å¿ƒæ¨¡å— ---
from router import RouterAgent
from graphrag import LightGraphRAG
from codez_gen import CodeGenAgent
from code_revise import CodeReviseAgent
from utils import quick_validate_mermaid, preprocess_multi_files
from document_reader import DocumentAnalyzer
from project_manager import ProjectManager
from git_loader import GitHubLoader
from style_agent import StyleAgent

# --- é…ç½® ---
PROJECTS_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.projects"))
DEFAULT_PROJECT = "default"

# --- å…¨å±€é¡¹ç›®ç®¡ç†å™¨ ---

project_manager = ProjectManager(DEFAULT_PROJECT,PROJECTS_ROOT)

# --- åˆå§‹åŒ– FastAPI ---
app = FastAPI(title="Smart Mermaid Backend (Project Managed)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 1. åˆå§‹åŒ– Agents ---
print("ğŸš€ [Backend] æ­£åœ¨å¯åŠ¨åç«¯å¼•æ“ï¼ŒåŠ è½½ Agents...")
default_graph_db = os.path.join(project_manager.get_project_dir(DEFAULT_PROJECT), "graph_db")

try:
    rag_engine = LightGraphRAG(persist_dir=default_graph_db)
    router_agent = RouterAgent(model_name="deepseek-chat", learn_mode=False)
    code_gen_agent = CodeGenAgent(model_name="deepseek-chat")
    code_revise_agent = CodeReviseAgent(
        mistake_file_path="./knowledge/experience/mistakes.json", 
        model_name="deepseek-chat"
    )
    doc_analyzer = DocumentAnalyzer() 
    style_agent = StyleAgent(model_name="deepseek-chat")
    print("âœ… [Backend] å¼•æ“åŠ è½½å®Œæ¯•ï¼")
except Exception as e:
    print(f"âŒ [Backend] å¼•æ“åŠ è½½å¤±è´¥: {e}")


# --- ä»»åŠ¡çŠ¶æ€ç®¡ç† ---
tasks = {}

def process_upload_background(task_id: str, file_location: str, project_name: str):
    """åå°ä»»åŠ¡ï¼šå¤„ç†æ–‡ä»¶å¹¶æ„å»ºå›¾è°±"""
    time.sleep(2) 
    
    try:
        tasks[task_id] = {"status": "processing", "message": "æ–‡ä»¶å·²æ¥æ”¶..."}
        project_manager.update_file_status(task_id, "uploaded", "ç­‰å¾…ç”Ÿæˆæ—¶å¤„ç†")
        
        print(f"ğŸ”„ [Task {task_id}] æ–‡ä»¶å·²å°±ç»ª: {os.path.basename(file_location)}")
        
        # æ³¨æ„ï¼šä¸å†æ­¤å¤„è‡ªåŠ¨æ„å»ºå›¾è°±ï¼Œè€Œæ˜¯æ¨è¿Ÿåˆ°ç”Ÿæˆé˜¶æ®µç»Ÿä¸€å¤„ç†å¤šæ–‡ä»¶
        
        tasks[task_id] = {"status": "success", "message": "æ–‡ä»¶å°±ç»ª"}
        project_manager.update_file_status(task_id, "success", "æ–‡ä»¶å°±ç»ª")
        
    except Exception as e:
        tasks[task_id] = {"status": "error", "message": str(e)}
        project_manager.update_file_status(task_id, "error", str(e))
        print(f"âŒ [Task {task_id}] å¤„ç†å¤±è´¥: {e}")


# --- 2. Request Models ---

class GenerateRequest(BaseModel):
    text: str
    diagramType: str = "auto"
    aiConfig: Optional[Dict[str, Any]] = None
    useGraph: bool = True 
    useFileContext: bool = True # æ˜¯å¦ä½¿ç”¨æ–‡ä»¶ä¸Šä¸‹æ–‡
    useHistory:bool = False
    useMistakes:bool = False
    richness:float = 0.5

class FixRequest(BaseModel):
    mermaidCode: str
    errorMessage: str

class PasswordRequest(BaseModel):
    password: str

class ConfigUpdateRequest(BaseModel):
    apiKey: str
    apiUrl: str
    modelName: str

class ProjectCreateRequest(BaseModel):
    name: str

class ProjectSwitchRequest(BaseModel):
    name: str

class HistoryEntry(BaseModel):
    id: Optional[str] = None
    query: str
    code: str
    diagramType: str = "auto"
    timestamp: Optional[str] = None

class OptimizeRequest(BaseModel):
    code: str
    instruction: str
    aiConfig: Optional[Dict[str, Any]] = None
    accessPassword: Optional[str] = None
    selectedModel: Optional[str] = None

class GitHubAnalysisRequest(BaseModel):
    repoUrl: str
    diagramType: str = "auto"
    aiConfig: Optional[Dict[str, Any]] = None
    richness: float = 0.5

class StyleGenRequest(BaseModel):
    description: str


# ==========================================
# === æ ¸å¿ƒé€»è¾‘å°è£… (Refactored Helpers) ===
# ==========================================

def run_code_revision_loop(
    initial_code: str, 
    revise_agent: CodeReviseAgent,
    user_query: Optional[str] = None,
    router_agent_instance: Optional[RouterAgent] = None,
    use_mistakes: bool = False,
    status_callback: Optional[Callable[[str], None]] = None
):
    """
    é€šç”¨ä»£ç ä¿®å¤å¾ªç¯ï¼šæ ¡éªŒ -> å¤±è´¥ -> è®°å½• -> ä¿®å¤ (æœ€å¤š3æ¬¡)
    :param initial_code: åˆå§‹ç”Ÿæˆçš„ä»£ç 
    :param revise_agent: ä¿®å¤ä»£ç†å®ä¾‹
    :param user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºæˆåŠŸåå­¦ä¹ ï¼‰
    :param router_agent_instance: è·¯ç”±ä»£ç†å®ä¾‹ï¼ˆç”¨äºæˆåŠŸåå­¦ä¹ ï¼‰
    :param use_mistakes: æ˜¯å¦ä½¿ç”¨é”™è¯¯æœ¬è¾…åŠ©ä¿®å¤
    :param status_callback: å¯é€‰çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–°å¤–éƒ¨çŠ¶æ€ï¼ˆå¦‚ GitHub ä»»åŠ¡æ¶ˆæ¯ï¼‰
    :return: (final_code, final_error)
    """
    current_code = initial_code
    max_retries = 3 
    attempt_history = []
    validation = {'valid': False, 'error': 'Not started'}

    print(f"   -> æ­£åœ¨æ ¡éªŒä»£ç è¯­æ³• (æœ€å¤§é‡è¯• {max_retries} æ¬¡)...")

    for i in range(max_retries + 1):
        print(f"   ğŸ” [ç¬¬ {i+1} æ¬¡æ ¡éªŒ] ...")
        validation = quick_validate_mermaid(current_code)
        
        if validation['valid']:
            print("   âœ… æ ¡éªŒé€šè¿‡")
            
            # 1. è®°å½•ç»éªŒ (å¦‚æœæ˜¯åœ¨ä¿®å¤è¿‡ç¨‹ä¸­æˆåŠŸçš„)
            if i > 0 and len(attempt_history) > 0 and revise_agent:
                try:
                    last_fail = attempt_history[-1]
                    revise_agent.record_mistake(last_fail["code"], last_fail["error"], current_code)
                    print("   ğŸ“š é”™è¯¯ä¿®å¤ç»éªŒå·²å½•å…¥")
                except Exception as e:
                    print(f"   âš ï¸ ç»éªŒå½•å…¥å¤±è´¥: {e}")
            
            # 2. Router å­¦ä¹  (å¦‚æœæä¾›äº† agent å’Œ query)
            try: 
                if router_agent_instance and user_query: 
                    router_agent_instance.learn_from_success(user_query, current_code)
            except: pass
            
            break 
        
        else:
            error_msg = validation['error']
            print(f"   âŒ æ ¡éªŒå¤±è´¥: {error_msg[:50]}...")
            
            if i == max_retries:
                print("   âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè‡ªåŠ¨ä¿®å¤")
                break
            
            attempt_history.append({"code": current_code, "error": error_msg})
            
            if revise_agent:
                msg = f"æ­£åœ¨è‡ªåŠ¨ä¿®å¤è¯­æ³•é”™è¯¯ ({i+1}/{max_retries})..."
                print(f"   ğŸ”§ {msg}")
                if status_callback:
                    status_callback(msg)

                current_code = revise_agent.revise_code(
                    current_code, 
                    error_message=error_msg, 
                    previous_attempts=attempt_history,
                    use_mistake_book=use_mistakes
                )
            else:
                print("   âš ï¸ CodeReviseAgent æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œä¿®å¤")
                break
    
    final_error = validation['error'] if not validation['valid'] else None
    return current_code, final_error

# ==========================================
# === ç®€å•çš„æ–‡ä»¶çŠ¶æ€ç®¡ç† (åŸºäºé¡¹ç›®ç›®å½•) ===
# ==========================================


def build_file_context(user_query: str, use_graph: bool, use_file: bool) -> str:
    """
    æ„å»ºä¸Šä¸‹æ–‡ï¼šåŸºäº ProjectManager çš„ç»Ÿä¸€çŠ¶æ€ç®¡ç†
    """
    context = ""
    
    # 1. è·å–é¡¹ç›®è·¯å¾„å’Œæ–‡ä»¶è®°å½•
    project_dir = project_manager.get_project_dir() 
    upload_dir = os.path.join(project_dir, "uploads")
    
    # è·å–â€œå”¯ä¸€çœŸç†â€ï¼šProjectManager é‡Œçš„è®°å½•
    file_records = project_manager.get_file_records() 
    # å»ºç«‹ filename -> record çš„æ˜ å°„ï¼Œæ–¹ä¾¿åç»­æŸ¥æ‰¾
    record_map = {rec['filename']: rec for rec in file_records}
    
    # æ‰«æå®é™…å­˜åœ¨çš„ç‰©ç†æ–‡ä»¶
    _, text_files, blob_files = preprocess_multi_files(upload_dir, project_dir)
    all_current_files = text_files + blob_files
    
    if use_file and len(all_current_files) > 0:
        if use_graph:
            print(f"   -> ğŸ”µ Mode: GraphRAG (Project: {project_manager.current_project})")
            
            # 3. æ‰¾å‡ºéœ€è¦æ›´æ–°åˆ°å›¾è°±çš„æ–‡ä»¶
            files_to_update = []
            
            for fpath in all_current_files:
                fname = os.path.basename(fpath)
                current_mtime = os.path.getmtime(fpath) # ç‰©ç†æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´
                
                record = record_map.get(fname)
                
                # åˆ¤æ–­é€»è¾‘ï¼š
                # 1. å¦‚æœ ProjectManager é‡Œæ²¡è®°å½•ï¼ˆå¯èƒ½æ˜¯æ‰‹åŠ¨å¤åˆ¶è¿›å»çš„ï¼‰ï¼Œæš‚ä¸å¤„ç†æˆ–å¼ºåˆ¶æ›´æ–°
                # 2. å¦‚æœè®°å½•é‡Œæ²¡æœ‰ 'last_graph_sync' å­—æ®µï¼ˆè¯´æ˜ä¸Šä¼ äº†ä½†ä»æœªæ„å»ºè¿‡å›¾è°±ï¼‰-> éœ€è¦æ›´æ–°
                # 3. å¦‚æœç‰©ç†æ–‡ä»¶æ¯”è®°å½•çš„æ—¶é—´æ–°ï¼ˆè¯´æ˜æ–‡ä»¶è¢«ä¿®æ”¹è¿‡ï¼‰-> éœ€è¦æ›´æ–°
                
                needs_update = False
                if not record:
                    # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼Œé™¤éæ‰‹åŠ¨æ“ä½œäº†æ–‡ä»¶å¤¹
                    print(f"   âš ï¸ Warning: File {fname} found on disk but not in ProjectManager.")
                    continue 
                
                last_sync = record.get("last_graph_sync", 0) # é»˜è®¤ä¸º 0
                
                if current_mtime > last_sync:
                    needs_update = True
                
                if needs_update:
                    files_to_update.append((fpath, record))

            # 4. å¦‚æœæœ‰å˜åŠ¨ï¼Œæ‰§è¡Œå¢é‡æ„å»º
            if files_to_update:
                print(f"   -> å‘ç° {len(files_to_update)} ä¸ªæ–‡ä»¶éœ€è¦åŒæ­¥åˆ°å›¾è°±...")
                graph_input_path = os.path.join(upload_dir, "graph_full_context.md")
                new_content_buffer = ""
                
                for fpath, record in files_to_update:
                    fname = os.path.basename(fpath)
                    try:
                        # --- è¯»å–å†…å®¹ ---
                        if fpath in text_files:
                            with open(fpath, 'r', encoding='utf-8') as f:
                                content = f.read()
                            new_content_buffer += f"\n\n### File: {fname}\n{content}\n"
                        else:
                            blob_desc = doc_analyzer.analyze(
                                fpath, 
                                prompt="è¯·è¯¦ç»†æè¿°è¯¥æ–‡ä»¶çš„å†…å®¹ï¼Œä»¥ä¾¿æ„å»ºå‡†ç¡®çš„çŸ¥è¯†å›¾è°±ã€‚", 
                                max_token_limit=2400
                            )
                            new_content_buffer += f"\n\n### File: {fname}\nContent Description:\n{blob_desc}\n"
                        
                        # --- å…³é”®ä¿®æ”¹ï¼šæ›´æ–° ProjectManager è®°å½• ---
                        # è®°å½•å½“å‰æ—¶é—´æˆ³ï¼Œå¹¶æ ‡è®°çŠ¶æ€ä¸º "indexed" (å·²ç´¢å¼•)
                        project_manager.update_file_info(
                            record["id"], 
                            {
                                "last_graph_sync": os.path.getmtime(fpath),
                                "status": "indexed", # æˆ–è€…ä¿æŒ "success"
                                "message": "å·²åŒæ­¥è‡³çŸ¥è¯†åº“"
                            }
                        )
                        
                    except Exception as e:
                        print(f"      âŒ å¤„ç†å¤±è´¥ {fname}: {e}")
                        project_manager.update_file_info(
                            record["id"], 
                            {"status": "error", "message": f"å›¾è°±æ„å»ºå¤±è´¥: {str(e)}"}
                        )

                # å†™å…¥ Graph md å¹¶è§¦å‘æ„å»º
                if new_content_buffer:
                    with open(graph_input_path, "a", encoding="utf-8") as f:
                        f.write(new_content_buffer)
                    
                    rag_engine.build_graph(graph_input_path)
            else:
                print("   -> âœ¨ å›¾è°±å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€é‡å»ºã€‚")

            # æœç´¢å›¾è°±
            print("   -> Searching Knowledge Graph...")
            context = rag_engine.search(user_query, top_k=3)

        # --- åˆ†æ”¯ B: ç›´æ¥å¤šæ–‡ä»¶æ¨¡å¼ (No Graph) ---
        else:
            print("   -> ğŸŸ  Mode: Direct Analysis (All Files)")
            
            all_targets = text_files + blob_files
            count = len(all_targets)
            token_budget = 1200
            limit_per_file = max(100, token_budget // count) if count > 0 else 1200
            
            file_contexts = []
            print(f"   -> Processing {count} files (Limit: ~{limit_per_file} tokens/file)...")
            
            for fpath in all_targets:
                try:
                    analysis = doc_analyzer.analyze(
                        fpath, 
                        prompt=f"Briefly explain this file's relevance to: {user_query}", 
                        max_token_limit=limit_per_file
                    )
                    print("æ–‡æ¡£æå–æˆåŠŸ......")
                    file_contexts.append(f"### File: {os.path.basename(fpath)}\nSummary:\n{analysis}\n")
                except Exception as e:
                    print(f"Error analyzing {fpath}: {e}")
            
            context = "\n".join(file_contexts)
    
    else:
        print("   -> âšª Mode: Pure Text (No File Context)")
        context = "" 

    return context

# --- 3. Routes ---

# === é¡¹ç›®ç®¡ç†æ¥å£ ===

@app.get("/api/projects")
async def list_projects():
    return {
        "projects": project_manager.list_projects(), 
        "current": project_manager.current_project
    }

@app.post("/api/projects")
async def create_project(req: ProjectCreateRequest):
    if not re.match(r'^[a-zA-Z0-9_-]+$', req.name):
        return {"status": "error", "message": "é¡¹ç›®åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦"}
    
    if req.name in project_manager.list_projects():
        return {"status": "error", "message": "é¡¹ç›®å·²å­˜åœ¨"}
    
    project_manager.ensure_project_exists(req.name)
    return {"status": "success", "message": f"é¡¹ç›® {req.name} å·²åˆ›å»º"}

@app.post("/api/projects/switch")
async def switch_project(req: ProjectSwitchRequest):
    try:
        if req.name == project_manager.current_project:
            return {"status": "success", "current": req.name, "message": "Already on this project"}

        new_dir = project_manager.switch_project(req.name)
        print(f"ğŸ”„ [Project] åˆ‡æ¢è‡³: {req.name}")
        
        new_graph_db = os.path.join(new_dir, "graph_db")
        rag_engine.reload_db(new_graph_db)
        
        return {"status": "success", "current": req.name}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# === æ–‡ä»¶åˆ—è¡¨æ¥å£ ===
@app.get("/api/files")
async def list_files():
    return project_manager.get_file_records()

@app.delete("/api/files/{file_id}")
async def delete_file(file_id: str):
    project_manager.remove_file_record(file_id)
    return {"status": "success"}

# === å›¾è°±æ•°æ®æ¥å£ ===
@app.get("/api/graph/data")
async def get_graph_data():
    return rag_engine.get_graph_snapshot()

# === å†å²è®°å½•æ¥å£ ===

@app.get("/api/history")
async def get_history():
    p_dir = project_manager.get_project_dir()
    hist_file = os.path.join(p_dir, "history.json")
    try:
        if os.path.exists(hist_file):
            with open(hist_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return []
    except Exception:
        return []

@app.post("/api/history")
async def add_history(entry: HistoryEntry):
    p_dir = project_manager.get_project_dir()
    hist_file = os.path.join(p_dir, "history.json")
    
    new_item = entry.dict()
    if not new_item.get("id"):
        new_item["id"] = str(int(time.time() * 1000))
    if not new_item.get("timestamp"):
        new_item["timestamp"] = datetime.now().isoformat()
        
    try:
        current_data = []
        if os.path.exists(hist_file):
            with open(hist_file, "r", encoding="utf-8") as f:
                try: current_data = json.load(f)
                except: current_data = []
        
        current_data.insert(0, new_item)
        
        with open(hist_file, "w", encoding="utf-8") as f:
            json.dump(current_data, f, ensure_ascii=False, indent=2)
            
        return {"status": "success", "entry": new_item}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.delete("/api/history/{entry_id}")
async def delete_history(entry_id: str):
    p_dir = project_manager.get_project_dir()
    hist_file = os.path.join(p_dir, "history.json")
    try:
        if os.path.exists(hist_file):
            with open(hist_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            new_data = [item for item in data if item.get("id") != entry_id]
            with open(hist_file, "w", encoding="utf-8") as f:
                json.dump(new_data, f, ensure_ascii=False, indent=2)
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "message": str(e)}
    
@app.delete("/api/history")
async def clear_history():
    p_dir = project_manager.get_project_dir()
    hist_file = os.path.join(p_dir, "history.json")
    try:
        with open(hist_file, "w", encoding="utf-8") as f:
            json.dump([], f)
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# === ç³»ç»Ÿé…ç½®æ¥å£ ===
@app.post("/api/system/config")
async def update_system_config(config: ConfigUpdateRequest):
    print(f"ğŸ”„ [System] æ”¶åˆ°é…ç½®æ›´æ–°è¯·æ±‚: {config.modelName} @ {config.apiUrl}")
    try:
        config_dict = config.dict()
        if 'router_agent' in globals(): router_agent.reload_llm_config(config_dict)
        if 'code_gen_agent' in globals(): code_gen_agent.reload_llm_config(config_dict)
        if 'code_revise_agent' in globals(): code_revise_agent.reload_llm_config(config_dict)
        return {"status": "success", "message": "AIé…ç½®å·²çƒ­æ›´æ–°"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# === æ–‡ä»¶ä¸Šä¼ æ¥å£ ===

@app.post("/api/upload")
async def upload_file(
    background_tasks: BackgroundTasks, 
    file: UploadFile = File(...),
    autoBuild: bool = Form(True) 
):
    try:
        upload_dir = os.path.join(project_manager.get_project_dir(), "uploads")
        os.makedirs(upload_dir, exist_ok=True)
        
        file_location = os.path.join(upload_dir, file.filename)
        
        with open(file_location, "wb") as f:
            while True:
                chunk = await file.read(1024 * 1024)
                if not chunk:
                    break
                await run_in_threadpool(f.write, chunk)
            
        task_id = str(uuid.uuid4())
        print(f"ğŸ“‚ [Upload] æ”¶åˆ°æ–‡ä»¶: {file.filename}, ID: {task_id}, AutoBuild: {autoBuild}")
        
        initial_status = "pending" if autoBuild else "uploaded"
        initial_msg = "æ–‡ä»¶ç­‰å¾…å¤„ç†..." if autoBuild else "æ–‡ä»¶å·²ä¿å­˜ (å¾…åˆ†æ)"
        
        tasks[task_id] = {
            "status": initial_status,
            "message": initial_msg,
            "filename": file.filename,
            "timestamp": time.time(),
            "location": file_location 
        }
        
        file_record = {
            "id": task_id,
            "filename": file.filename,
            "status": initial_status,
            "message": initial_msg,
            "timestamp": datetime.now().isoformat(),
            "location": file_location, 
            "size": 0 
        }
        project_manager.add_file_record(file_record)
        
        if autoBuild:
            background_tasks.add_task(
                process_upload_background, 
                task_id, 
                file_location, 
                project_manager.current_project
            )
        
        return {
            "status": "success", 
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ" + ("ï¼Œå·²è¿›å…¥å¤„ç†é˜Ÿåˆ—" if autoBuild else "ï¼Œç­‰å¾…ä½¿ç”¨"),
            "taskId": task_id,
            "filename": file.filename
        }
    except Exception as e:
        print(f"ğŸ”¥ Upload Error: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    task = tasks.get(task_id)
    if not task:
        return {"status": "error", "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}
    return task

# === æ ¸å¿ƒç”Ÿæˆæ¥å£ (å¢å¼ºç‰ˆï¼šæ”¯æŒå¤šæ–‡ä»¶åˆ†è·¯å¤„ç†) ===
@app.post("/api/generate-mermaid")
async def generate_mermaid(request: GenerateRequest):
    user_query = request.text
    print(f"\nâš¡ [Generate] æ”¶åˆ°è¯·æ±‚: {user_query[:50]}... | Graph: {request.useGraph} | File: {request.useFileContext}")

    try:
        # 1. è°ƒç”¨å°è£…å¥½çš„ä¸Šä¸‹æ–‡æ„å»ºå‡½æ•°
        context = build_file_context(user_query, request.useGraph, request.useFileContext)

        # 2. Router è°ƒåº¦ä¸­å¿ƒ
        print("   -> Router æ­£åœ¨åˆ¶å®šç­–ç•¥...")
        
        if request.diagramType == "auto":
            route_res = router_agent.route_and_analyze(user_content=context, user_target=user_query, use_experience=request.useHistory)
        else:
            print(f"   -> ç”¨æˆ·å¼ºåˆ¶æŒ‡å®šç±»å‹: {request.diagramType}")
            route_res = router_agent.analyze_specific_mode(
                user_content=context, 
                user_target=user_query, 
                specific_type=request.diagramType,
                use_experience=request.useHistory
            )
            
        prompt_file = route_res.get("target_prompt_file", "flowchart.md")
        logic_analysis = route_res.get("analysis_content", "")
        
        print(f"   -> ç›®æ ‡ Prompt: {prompt_file}")
        
        # 3. ä»£ç ç”Ÿæˆ
        print("   -> æ­£åœ¨ç”Ÿæˆä»£ç ...")
        initial_code = code_gen_agent.generate_code(logic_analysis, prompt_file=prompt_file, richness=request.richness)
        
        # 4. è°ƒç”¨å°è£…å¥½çš„å¾ªç¯ä¿®å¤é€»è¾‘
        final_code, final_error = run_code_revision_loop(
            initial_code=initial_code,
            revise_agent=code_revise_agent,
            user_query=user_query,
            router_agent_instance=router_agent,
            use_mistakes=request.useMistakes
        )

        return {"mermaidCode": final_code, "error": final_error}

    except Exception as e:
        print(f"ğŸ”¥ [Generate] å¤„ç†å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return {"mermaidCode": "", "error": str(e)}

# === GitHub åˆ†ææ¥å£ ===

def process_github_background(task_id: str, repo_url: str, diagram_type: str, richness: float):
    """GitHub åˆ†æçš„åå°ä»»åŠ¡é€»è¾‘"""
    try:
        # 1. æ›´æ–°çŠ¶æ€ï¼šå…‹éš†ä¸­
        tasks[task_id].update({"status": "processing", "message": "æ­£åœ¨å…‹éš†ä»“åº“..."})
        
        project_dir = project_manager.get_project_dir()
        # æºä»£ç å­˜å‚¨è·¯å¾„ (ä¸ uploads éš”ç¦»)
        loader = GitHubLoader(base_dir=os.path.join(project_dir, "repos"))
        upload_dir = os.path.join(project_dir, "uploads")
        os.makedirs(upload_dir, exist_ok=True)
        
        # 2. ä¸‹è½½ä»£ç 
        print(f"   -> [Task {task_id}] Cloning {repo_url}...")
        repo_path = loader.clone_repo(repo_url)
        repo_name = os.path.basename(repo_path)
        
        # 3. åˆ†æç»“æ„
        tasks[task_id].update({"message": "æ­£åœ¨åˆ†ææ–‡ä»¶ç»“æ„..."})
        files_map = loader.classify_files(repo_path)
        tree_structure = loader.generate_tree_structure(repo_path)
        
        # 4. æ·±åº¦åˆ†ææ ¸å¿ƒä»£ç 
        source_files = files_map['source_code']
        # ä½¿ç”¨æ™ºèƒ½ç­›é€‰
        max_files_to_analyze = 30 
        selected_files = loader.smart_select_files(source_files, max_files=max_files_to_analyze)
        
        analysis_results = []
        count = 0
        ignored_files = set(source_files) - set(selected_files)
        
        print(f"   -> Smart selected {len(selected_files)} files from {len(source_files)} total sources.")
        
        for file_path in selected_files:
            count += 1
            tasks[task_id].update({"message": f"æ­£åœ¨æ·±åº¦é˜…è¯» ({count}/{len(selected_files)}): {os.path.basename(file_path)}"})
            
            try:
                res = doc_analyzer.analyze_code_file(file_path, project_root=repo_path)
                analysis_results.append(res)
            except Exception as e:
                print(f"      âŒ Skipped {os.path.basename(file_path)}: {e}")
            
        # 5. ç»„è£… Context
        full_context = (
            f"# GitHub Repository Analysis: {repo_name}\n\n"
            f"> Source URL: {repo_url}\n"
            f"> Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            f"## 1. Directory Structure\n"
            f"```\n{tree_structure}\n```\n\n"
            f"## 2. Core Logic Analysis\n"
            f"{''.join(analysis_results)}\n\n"
            f"## 3. Supplementary Info\n"
            f"Total files scanned: {len(source_files)}. Files fully analyzed: {len(selected_files)}.\n"
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        summary_filename = f"{repo_name}.md"
        summary_file_path = os.path.join(upload_dir, summary_filename)
        with open(summary_file_path, "w", encoding="utf-8") as f:
            f.write(full_context)
        print(f"   ğŸ’¾ [Save] Context saved to: {summary_filename}")

        try:
            summary_record = {
                "id": str(uuid.uuid4()),
                "filename": summary_filename, 
                "status": "success",
                "message": "GitHub æ™ºèƒ½åˆ†ææŠ¥å‘Š",
                "timestamp": datetime.now().isoformat(),
                "location": summary_file_path,
                "size": len(full_context),
                "isGithubAnalysis": True
            }
            project_manager.add_file_record(summary_record)
        except Exception as e:
            print(f"   âš ï¸ Failed to register summary file: {e}")

        user_query = f"Analyze the architecture of the GitHub repository '{repo_name}'. Use the Directory Tree to understand the full scope, and the Core File Analysis to understand the specific logic implementation."
        
        # 6. ç”Ÿæˆå›¾è¡¨
        tasks[task_id].update({"message": "AI æ­£åœ¨æ„å»ºå›¾è¡¨é€»è¾‘..."})
        
        if diagram_type == "auto":
            route_res = router_agent.route_and_analyze(user_content=full_context, user_target=user_query)
        else:
            route_res = router_agent.analyze_specific_mode(
                user_content=full_context, 
                user_target=user_query, 
                specific_type=diagram_type
            )
            
        prompt_file = route_res.get("target_prompt_file", "classDiagram.md")
        logic_analysis = route_res.get("analysis_content", "")
        
        tasks[task_id].update({"message": "æ­£åœ¨ç”Ÿæˆ Mermaid ä»£ç ..."})
        initial_code = code_gen_agent.generate_code(logic_analysis, prompt_file=prompt_file, richness=richness)
        
        # 7. Code Revise (ä½¿ç”¨å°è£…å‡½æ•°ï¼Œå¸¦çŠ¶æ€æ›´æ–°å›è°ƒ)
        def update_status(msg):
            tasks[task_id].update({"message": msg})

        final_code, final_error = run_code_revision_loop(
            initial_code=initial_code,
            revise_agent=code_revise_agent,
            user_query=None, # GitHub ä»»åŠ¡æš‚ä¸è§¦å‘ Router å­¦ä¹ 
            status_callback=update_status
        )
        
        # ä¿å­˜å†å²è®°å½•
        try:
            hist_entry = {
                "id": str(int(time.time() * 1000)),
                "query": f"GitHub Analysis: {repo_name}",
                "code": final_code,
                "diagramType": diagram_type,
                "timestamp": datetime.now().isoformat(),
                "analysisSummary": logic_analysis
            }
            p_dir = project_manager.get_project_dir()
            hist_file = os.path.join(p_dir, "history.json")
            
            current_hist = []
            if os.path.exists(hist_file):
                with open(hist_file, "r", encoding="utf-8") as f:
                    try: current_hist = json.load(f)
                    except: current_hist = []
            
            current_hist.insert(0, hist_entry)
            with open(hist_file, "w", encoding="utf-8") as f:
                json.dump(current_hist, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"   âš ï¸ Failed to save history: {e}")

        # 8. ä»»åŠ¡å®Œæˆ
        print(f"âœ… [Task {task_id}] GitHub åˆ†æå®Œæˆ")
        tasks[task_id] = {
            "status": "success",
            "message": "åˆ†æå®Œæˆ",
            "result": {  
                "mermaidCode": final_code,
                "error": final_error,
                "analysisSummary": logic_analysis
            }
        }

    except Exception as e:
        print(f"âŒ [Task {task_id}] Failed: {e}")
        tasks[task_id] = {"status": "error", "message": str(e)}

@app.post("/api/upload-github")
async def analyze_github(request: GitHubAnalysisRequest, background_tasks: BackgroundTasks):
    task_id = str(uuid.uuid4())
    print(f"\nâš¡ [GitHub] æ”¶åˆ°è¯·æ±‚ï¼Œåˆ›å»ºåå°ä»»åŠ¡ ID: {task_id}")
    tasks[task_id] = {
        "status": "pending",
        "message": "ä»»åŠ¡åˆå§‹åŒ–...",
        "type": "github",
        "repo": request.repoUrl
    }
    background_tasks.add_task(
        process_github_background,
        task_id,
        request.repoUrl,
        request.diagramType,
        request.richness
    )
    return {"status": "success", "taskId": task_id, "message": "åå°åˆ†æå·²å¯åŠ¨"}

@app.post("/api/optimize-mermaid")
async def optimize_mermaid(request: OptimizeRequest):
    print(f"\nâš¡ [Optimize] æ”¶åˆ°ä¼˜åŒ–è¯·æ±‚: {request.instruction[:50]}...")
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šæ‰§è¡Œä¼˜åŒ–
        current_code = code_revise_agent.optimize_code(request.code, request.instruction)
        
        # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨å°è£…çš„æ ¡éªŒ+ä¿®å¤é€»è¾‘
        final_code, final_error = run_code_revision_loop(
            initial_code=current_code,
            revise_agent=code_revise_agent,
            use_mistakes=False # ä¼˜åŒ–é€šå¸¸ä¸æŸ¥é”™è¯¯æœ¬ï¼Œè€Œæ˜¯åŸºäºæŒ‡ä»¤
        )
        
        return {
            "optimizedCode": final_code, 
            "error": final_error
        }

    except Exception as e:
        print(f"ğŸ”¥ [Optimize] å¤„ç†å¼‚å¸¸: {e}")
        return {"optimizedCode": request.code, "error": str(e)}

@app.post("/api/fix-mermaid")
async def fix_mermaid(request: FixRequest):
    # è°ƒç”¨å°è£…çš„å¾ªç¯ä¿®å¤é€»è¾‘
    final_code, final_error = run_code_revision_loop(
        initial_code=request.mermaidCode,
        revise_agent=code_revise_agent,
        use_mistakes=True # çº¯ä¿®å¤æ¨¡å¼å»ºè®®å¼€å¯é”™è¯¯æœ¬å­¦ä¹ 
    )
    return {"fixedCode": final_code, "error": final_error}

@app.get("/api/models")
async def get_models():
    return {
        "success": True,
        "models": [
            {"id": "deepseek-chat", "name": "DeepSeek V3 (Server)", "description": "Backend Default"},
            {"id": "deepseek-reasoner", "name": "DeepSeek R1 (Reasoning)", "description": "High Intelligence"}
        ]
    }

@app.post("/api/verify-password")
async def verify_password(req: PasswordRequest):
    return {"success": True, "message": "Access Granted"}

@app.post("/api/style/generate")
async def generate_graph_style(req: StyleGenRequest):
    print(f"ğŸ¨ [Style] æ”¶åˆ°æ ·å¼ç”Ÿæˆè¯·æ±‚: {req.description}")
    try:
        # è°ƒç”¨ StyleAgent
        result = style_agent.generate_style(req.description)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if result.get("error"):
            return {"status": "error", "message": result["error"]}
            
        return {"status": "success", "data": result}
    except Exception as e:
        print(f"ğŸ”¥ [Style] ç”Ÿæˆå¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)