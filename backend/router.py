import json
import os
from Agent import deepseek_agent
from rag import LocalKnowledgeBase
from typing import Dict, Any, List

class RouterAgent:
    def __init__(self, 
                 model_name: str = "deepseek-reasoner", 
                 learn_mode: bool = False,
                 experience_file: str = "./knowledge/experience/router.json"):
        
        print(f"--- åˆå§‹åŒ– RouterAgent (æ™ºèƒ½è¿›åŒ–ç‰ˆ) [æ¨¡åž‹: {model_name}] ---")
        self.llm = deepseek_agent(model_name=model_name)
        self.learn_mode = learn_mode
        self.experience_file = experience_file
        
        # 1. åˆå§‹åŒ– RAG å¼•æ“Ž (å¤ç”¨ LocalKnowledgeBase)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¤ç”¨ rag.py çš„èƒ½åŠ›ï¼Œå°†ç»éªŒæ± ä½œä¸º "QAçŸ¥è¯†" åŠ è½½
        self.rag = LocalKnowledgeBase("./.local_rag_db/router")
        
        # 2. åŠ è½½ç»éªŒ (å†·å¯åŠ¨)
        if os.path.exists(self.experience_file):
            print(f"ðŸ§  Router æ­£åœ¨åŠ è½½åŽ†å²ç»éªŒåº“: {self.experience_file}")
            # add_qa_mistakes æœ¬è´¨å°±æ˜¯åŠ è½½ list of {q, a}ï¼Œå®Œå…¨é€šç”¨
            # å®ƒä¼šè‡ªåŠ¨å¿½ç•¥ json é‡Œçš„ 'source_code' å­—æ®µï¼Œåªå­˜ q å’Œ a
            self.rag.add_qa_mistakes(self.experience_file)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç»éªŒåº“æ–‡ä»¶ï¼ŒRouter å°†ä»Žé›¶å¼€å§‹è¿è¡Œã€‚")

    def _load_prompt(self, file_path: str) -> str:
        """åŠ è½½å¤–éƒ¨æç¤ºè¯æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            return ""

    def route_and_analyze(self, user_content: str, user_target:str = "",) -> Dict[str, Any]:
        """
        æ ¸å¿ƒåŠŸèƒ½ï¼šåˆ†æžéœ€æ±‚ -> æ£€ç´¢ç»éªŒ -> åˆ¶å®šç­–ç•¥
        (å·²é‡æž„ï¼šå†…ç½® Promptï¼Œä¸å†ä¾èµ–å¤–éƒ¨æ–‡ä»¶ï¼Œç»Ÿä¸€ç®¡ç†å‚æ•°)
        """
        print(f"âš¡ Router æ­£åœ¨åˆ†æžéœ€æ±‚ (å­¦ä¹ æ¨¡å¼: {'å¼€å¯' if self.learn_mode else 'å…³é—­'})...")

        # 1. RAG æ£€ç´¢ï¼šçœ‹çœ‹ä»¥å‰æœ‰æ²¡æœ‰ç”»è¿‡ç±»ä¼¼çš„å›¾
        # search() è¿”å›žçš„æ˜¯ list of strings (å³ 'a'/è®¾è®¡æ€è·¯)
        retrieved_experiences = self.rag.search_score(query=user_target, top_k=10)
     
        # 2. æž„å»ºç»éªŒä¸Šä¸‹æ–‡ (Dynamic RAG Section)
        experience_section = ""
        if retrieved_experiences:
            print(f"   [RAG] è”æƒ³åˆ° {len(retrieved_experiences)} æ¡ç›¸å…³è®¾è®¡æ€è·¯")
            
            # æ‹¼æŽ¥å…·ä½“ç»éªŒåˆ—è¡¨
            context_list = "\n".join([f"{idx+1}. {exp}" for idx, exp in enumerate(retrieved_experiences)])
            
            # æž„é€ ç»éªŒæŒ‡ä»¤å—
            experience_section = (
                "\n\n"
                "### ðŸ§  CRITICAL REFERENCE (RAG MEMORY)\n"
                "The following are **SUCCESSFUL PAST STRATEGIES** retrieved from your memory bank.\n"
                "**INSTRUCTION**: You MUST prioritized these strategies. If a past case used a specific diagram type for a similar scenario, **COPY THAT CHOICE**.\n"
                "**Attention**: Pay more attention to the most popular strategies, for that is the most accepted, too.  "
                "**The diagram type you choose should be suitable for the user's requirement:**\n"
                "--------------------------------------------------\n"
                f"{context_list}\n"
            )
        else:
            print("   [RAG] æ— ç›¸å…³ç»éªŒï¼Œä½¿ç”¨é€šç”¨ç­–ç•¥ã€‚")

        
        # 3. æž„é€ å®Œæ•´ System Prompt (åŽŸ router.md + åŠ¨æ€é€»è¾‘)
        # åŒ…å«äº†å›¾è¡¨ç±»åž‹æ˜ å°„è¡¨å’Œè¾“å‡ºæ ¼å¼è¦æ±‚
        system_prompt = (
            "You are an intelligent **Visualization Orchestrator**.\n"
            "Your goal is to select the BEST Mermaid diagram type based on the user's request.\n\n"
            
            "### 1. Diagram Type Menu (Strict Mapping)\n"
            "Select the filename strictly from this list. Do NOT invent new filenames.\n\n"
            
            "**Structure **:\n"
            "- `flowchart.md`: Logic flows, algorithms, process steps. (Most Common)\n"
            "- `architecture.md`: Cloud/System high-level architecture.\n"
            "- `classDiagram.md`: OOP classes, data structures.\n"
            "- `entityRelationshipDiagram.md`: Database schemas (ERD).\n"
            "- `block.md`: Hardware layouts or simple block structures.\n\n"
            
            "**Behavior **:\n"
            "- `sequenceDiagram.md`: Interaction between services/actors over time.\n"
            "- `stateDiagram.md`: Lifecycle states, status transitions.\n"
            "- `userJourney.md`: User workflow steps.\n\n"
            
            "**Project & Data **:\n"
            "- `gantt.md`, `timeline.md`, `gitgraph.md`, `mindmap.md`\n"
            "- `pie.md`, `xyChart.md`, `quadrantChart.md`\n\n"
            f"{experience_section}\n"
            "**You should analyze the content according to the user's requirement**\n"
            "**You should contain as more details as you can in your output**\n"
            "### 2. Output Format (JSON Only)\n"
            "Output a SINGLE JSON object:\n"
            "{\n"
            "  \"reason\": \"Cite the specific RAG reference if used.\",\n"
            "  \"target_prompt_file\": \"filename.md\",\n"
            "  \"analysis_content\": \"Structured summary for the coder.\"\n"
            "}\n\n"
            
            
        )
        
        # 4. LLM å†³ç­–
        messages = [{"role": "user", "content": f"[User Requirement]:\n{user_target}\n\n[Context Content]:\n{user_content}"}]
        
        try:
            response_text = self.llm.chat(messages, system_prompt=system_prompt, json_mode=True)
            result = json.loads(response_text)
            
            # ç®€å•çš„åŽç¼€è¡¥å…¨
            if not result.get('target_prompt_file', '').endswith('.md'):
                result['target_prompt_file'] += ".md"
            
            return result
            
        except json.JSONDecodeError:
            print("Router JSON è§£æžå¤±è´¥ï¼Œå›žé€€é»˜è®¤ç­–ç•¥ã€‚")
            return {
                "target_prompt_file": "flowchart.md",
                "reason": "Fallback: JSON Parse Error",
                "analysis_content": user_content[:2000]
            }
        
    def analyze_specific_mode(self, user_content: str, user_target: str, specific_type: str) -> Dict[str, Any]:
        """
        ã€æ–°å¢žã€‘å®šå‘åˆ†æžæ¨¡å¼ï¼šå½“ç”¨æˆ·æ˜Žç¡®æŒ‡å®šå›¾è¡¨ç±»åž‹æ—¶è°ƒç”¨
        è·³è¿‡é€‰åž‹æ­¥éª¤ï¼Œç›´æŽ¥ç”Ÿæˆé’ˆå¯¹è¯¥å›¾è¡¨çš„åˆ†æžå†…å®¹ã€‚
        """
        print(f"âš¡ Router è¿›å…¥å®šå‘åˆ†æžæ¨¡å¼ -> ç›®æ ‡ç±»åž‹: {specific_type}\n")
        # 1. ä¾ç„¶å°è¯•æ£€ç´¢ç›¸å…³ç»éªŒ (å¯èƒ½åŒ…å«é’ˆå¯¹è¯¥ç‰¹å®šå›¾è¡¨çš„ç”»æ³•æŠ€å·§)
        retrieved_experiences = self.rag.search_score(query=user_target, top_k=5)
        experience_context = ""
        if retrieved_experiences:
            print(f"   [RAG] è”æƒ³åˆ° {len(retrieved_experiences)} æ¡ç›¸å…³è®¾è®¡æ€è·¯")
            experience_context = "\n### Reference Design Strategies (From Past Success):\n"
            for idx, exp in enumerate(retrieved_experiences):
                experience_context += f"{idx+1}. {exp}\n"
        else:
            print("   [RAG] æ— ç›¸å…³ç»éªŒï¼Œä½¿ç”¨é€šç”¨ç­–ç•¥ã€‚")
        if retrieved_experiences:
            # å¦‚æžœæœ‰ç»éªŒï¼Œå°±åŠ ä¸€æ®µâ€œç‹ è¯â€
            experience_instruction = (
                "\n\n"
                "### ðŸ§  CRITICAL REFERENCE (RAG MEMORY)\n"
                "The following are **SUCCESSFUL PAST STRATEGIES** retrieved from your memory bank.\n"
                f"**INSTRUCTION**: You can learn only from the {specific_type} strategies, .\nOther type of diagram has little value to learn from.\n"
                "--------------------------------------------------\n"
            )
            # æ‹¼è£…ï¼šæŒ‡ä»¤ + å…·ä½“çš„ç»éªŒåˆ—è¡¨
            experience_section = experience_instruction + experience_context
        else:
            experience_section = ""
        # 2. æž„é€ å®šå‘ Prompt
        system_prompt = (
            f"You are a Visualization Expert. The user has EXPLICITLY requested a '{specific_type}' diagram.\n"
            f"### INSTRUCTIONS:\n"
            f"1. Analyze the [User Content] and [User Requirement].\n"
            f"2. Extract the key entities, relationships, or steps needed to build a high-quality {specific_type}.\n"
            f"3. Do NOT suggest other diagram types.\n"
            f"4. Output JSON strictly.\n\n"
            f"{experience_section}"
            f"### OUTPUT FORMAT (JSON):\n"
            f"{{\n"
            f"  \"reason\": \"User manually selected {specific_type}.\",\n"
            f"  \"target_prompt_file\": \"{specific_type}.md\",\n"
            f"  \"analysis_content\": \"...Structured analysis summary suitable for generating {specific_type} code...\"\n"
            f"}}"
        )

        messages = [{"role": "user", "content": f"[User Requirement]: {user_target}\n\n[Context Content]:\n{user_content}"}]

        try:
            response_text = self.llm.chat(messages, system_prompt=system_prompt, json_mode=True)
            result = json.loads(response_text)
            
            # å¼ºåˆ¶ä¿®æ­£æ–‡ä»¶åï¼Œé˜²æ­¢LLMå¹»è§‰
            target_file = f"{specific_type}.md"
            result['target_prompt_file'] = target_file
            
            return result
        except Exception as e:
            print(f"Router å®šå‘åˆ†æžå¤±è´¥: {e}ï¼Œä½¿ç”¨åŽŸå§‹å†…å®¹ä½œä¸ºåˆ†æžç»“æžœ")
            return {
                "target_prompt_file": f"{specific_type}.md",
                "reason": "Fallback: Analysis Failed",
                "analysis_content": f"Requirement: {user_target}\nContext: {user_content[:1500]}"
            }

    def learn_from_success(self, user_query: str, valid_code: str):
        """
        ã€è¿›åŒ–æŽ¥å£ã€‘å½“ App ç¡®è®¤ä»£ç ç”ŸæˆæˆåŠŸåŽè°ƒç”¨ã€‚
        æç‚¼æœ¬æ¬¡æˆåŠŸçš„ {Q, A, Source} å¹¶å­˜å…¥åº“ã€‚
        """
        if not self.learn_mode:
            return

        print("ðŸ§  Router æ­£åœ¨ä»Žæœ¬æ¬¡æˆåŠŸæ¡ˆä¾‹ä¸­å­¦ä¹  (Experience Consolidation)...")
        
        # 1. LLM æç‚¼
        system_prompt = (
            "You are an Experience Extractor. Analyze the User Query and the Generated Mermaid Code.\n"
            "Extract a generic Experience Pair in JSON:\n"
            "{\n"
            "  \"q\": \"Abstract Scenario (e.g., Microservice Trace)\",\n"
            "  \"a\": \"Design Strategy (e.g., Use sequenceDiagram with activation bars...)\"\n"
            "}\n"
            "Note: 'q' should cover the intent, 'a' should cover the visualization technique."
        )
        
        user_msg = f"User Query:\n{user_query}\n\nGenerated Code:\n{valid_code[:1000]}..." # æˆªæ–­é˜²æ­¢å¤ªé•¿
        
        try:
            response = self.llm.chat([{"role": "user", "content": user_msg}], system_prompt=system_prompt, json_mode=True)
            data = json.loads(response)
            
            new_q = data.get("q")
            new_a = data.get("a")
            
            if new_q and new_a:
                # 2. æž„é€ å®Œæ•´è®°å½• (å«æºç æº¯æº)
                new_entry = {
                    "q": new_q,
                    "a": new_a,
                    "source_code": valid_code # ä¿ç•™æºç ä½œä¸ºæ¡ˆåº•
                }
                
                # 3. æŒä¹…åŒ–å­˜å‚¨ (JSON)
                self._save_to_disk(new_entry)
                
                # 4. è¿è¡Œæ—¶çƒ­æ›´æ–° (RAG)
                # åªéœ€è¦ q å’Œ a å³å¯æ£€ç´¢
                self.rag.add_single_qa(new_q, new_a, source="runtime_learning")
                print(f"âœ¨ Router ç»éªŒå€¼ +1: {new_q}")
                
        except Exception as e:
            print(f"Router å­¦ä¹ å¤±è´¥: {e}")

    def _save_to_disk(self, new_entry: Dict[str, Any]):
        """è¿½åŠ å†™å…¥ JSON æ–‡ä»¶"""
        current_data = []
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.experience_file), exist_ok=True)
        
        if os.path.exists(self.experience_file):
            try:
                with open(self.experience_file, 'r', encoding='utf-8') as f:
                    current_data = json.load(f)
            except:
                current_data = []
        
        # ç®€å•çš„æŸ¥é‡ (åŸºäºŽ Q)
        # å®žé™…ç”Ÿäº§ä¸­å¯èƒ½å…è®¸åŒä¸€ä¸ª Q æœ‰å¤šç§ Aï¼Œè¿™é‡Œç®€å•èµ·è§åŽ»é‡
        for item in current_data:
            if item.get('q') == new_entry['q']:
                return # å·²å­˜åœ¨ç±»ä¼¼åœºæ™¯ï¼Œæš‚ä¸é‡å¤å½•å…¥

        current_data.append(new_entry)
        
        with open(self.experience_file, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=2)
    def reload_llm_config(self, config: dict):
        """
        ã€çƒ­æ›´æ–°ã€‘æŽ¥æ”¶å‰ç«¯é…ç½®(é©¼å³°å‘½å)å¹¶æ›´æ–°åº•å±‚ LLM
        """
        # ä»Žå­—å…¸ä¸­æå–é…ç½®
        api_key = config.get("apiKey")
        api_url = config.get("apiUrl")
        model_name = config.get("modelName")
        
        # è°ƒç”¨åº•å±‚ Agent.py ä¸­å®šä¹‰çš„ update_config
        # è¿™é‡Œçš„ self.llm å¯¹åº” Agent/deepseek_agent å®žä¾‹
        if hasattr(self, 'llm'):
            self.llm.update_config(api_key=api_key, base_url=api_url, model_name=model_name)
            print(f"ðŸ”„ [{self.__class__.__name__}] LLMé…ç½®å·²é‡è½½ -> æ¨¡åž‹: {model_name}")

# --- å•å…ƒæµ‹è¯• ---
if __name__ == "__main__":
    # æµ‹è¯•åˆå§‹åŒ–
    router = RouterAgent(learn_mode=True)
    
    # æµ‹è¯•åˆ†æž
    req = "ç”»ä¸€ä¸ªTCPä¸‰æ¬¡æ¡æ‰‹çš„æ—¶åºå›¾"
    res = router.route_and_analyze(req)
    print("åˆ†æžç»“æžœ:", res.get("target_prompt_file"))
    
    # æµ‹è¯•å­¦ä¹ 
    code = "sequenceDiagram\nClient->>Server: SYN\nServer->>Client: SYN, ACK..."
    router.learn_from_success(req, code)