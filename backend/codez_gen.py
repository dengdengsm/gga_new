import os
from Agent import deepseek_agent, Message
from typing import List, Optional

class CodeGenAgent:
    def __init__(self, model_name: str = "deepseek-chat", prompt_dir: str = "./prompt/code_gen"):
        """
        åˆå§‹åŒ–ä»£ç ç”Ÿæˆ Agent
        æ”¯æŒä»Ž external Markdown æ–‡ä»¶åŠ è½½æç¤ºè¯ï¼Œå®žçŽ°å¤šç§å›¾è¡¨/ä»£ç çš„ç”Ÿæˆ
        """
        print(f"--- åˆå§‹åŒ– CodeGenAgent [æ¨¡åž‹: {model_name}] ---")
        self.llm = deepseek_agent(model_name=model_name)
        self.prompt_dir = prompt_dir
        
        # ç¡®ä¿æç¤ºè¯ç›®å½•å­˜åœ¨ï¼Œå¦‚æžœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œé¿å…æŠ¥é”™
        if not os.path.exists(self.prompt_dir):
            os.makedirs(self.prompt_dir, exist_ok=True)
            print(f"æç¤º: å·²è‡ªåŠ¨åˆ›å»ºæç¤ºè¯ç›®å½• {self.prompt_dir}")

    def _load_system_prompt(self, prompt_filename: str) -> str:
        """
        å†…éƒ¨æ–¹æ³•ï¼šä»Ž prompt æ–‡ä»¶å¤¹åŠ è½½ Markdown å†…å®¹
        :param prompt_filename: æ–‡ä»¶å (å¦‚ 'flowchart.md' æˆ– 'flowchart')
        :return: Prompt æ–‡æœ¬å†…å®¹
        """
        # å®¹é”™å¤„ç†ï¼šå¦‚æžœç”¨æˆ·æ²¡å†™ .md åŽç¼€ï¼Œè‡ªåŠ¨è¡¥å…¨
        if not prompt_filename.endswith(".md"):
            prompt_filename += ".md"
            
        file_path = os.path.join(self.prompt_dir, prompt_filename)
        
        if not os.path.exists(file_path):
            error_msg = f"é”™è¯¯: æç¤ºè¯æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ prompt ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨è¯¥æ–‡ä»¶ã€‚"
            print(error_msg)
            # è¿”å›žä¸€ä¸ªæžå…¶åŸºç¡€çš„é»˜è®¤ Prompt é˜²æ­¢ç¨‹åºç›´æŽ¥å´©æºƒ
            return "You are a code generator, please generate mermaid code for user's content."
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            return content

    def generate_code(self, input_text: str, prompt_file: str = "flowchart.md",richness:float = 0.5) -> str:
        """
        ã€é€šç”¨ç”ŸæˆæŽ¥å£ã€‘
        æ ¹æ®ä¼ å…¥çš„ prompt_file ä¸åŒï¼Œç”Ÿæˆä¸åŒç±»åž‹çš„ä»£ç  (æµç¨‹å›¾ã€æ€ç»´å¯¼å›¾ã€Pythonç»˜å›¾ç­‰)
        
        :param input_text: ç”¨æˆ·çš„éœ€æ±‚æè¿°æˆ–é€»è¾‘æ–‡æœ¬
        :param prompt_file: ä½äºŽ prompt æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶å
        """
        # 1. åŠ è½½ Prompt
        system_prompt = self._load_system_prompt(prompt_file)
        
        richness_requirement = f"""
            \n\n
            ### ðŸŽšï¸ DIAGRAM RICHNESS CONTROL (Target Level: {richness})
            The user has specified a richness parameter (0.0 - 1.0) to control the detail density of the generated diagram.
            Current Richness: **{richness}**

            **You MUST adapt your code generation strategy based on this value:**

            * **Low Richness (0.0 - 0.3) -> "High-Level Summary"**
                - **Focus**: Only show the main business.
                - **Contains NO More Than Ten Nodes**

            * **Medium Richness (0.4 - 0.7) -> "Standard Logic"**
                - **Focus**: Clear structural.
                - **Contains Strictly No More Than Twenty Nodes**

            * **High Richness (0.8 - 1.0) -> "Source Code Fidelity"**
                - **Focus**: A debugger-level view of the execution flow.
                - **Contains ENOUGH information in the Nodes**

            **Constraint**: Your output Mermaid code complexity MUST strictly match the richness level of **{richness}**.
            """
        
        system_prompt += richness_requirement
        # 2. æž„å»ºæ¶ˆæ¯
        messages: List[Message] = [
            {"role": "user", "content": f"[Requirements or content]:\n{input_text}"}
        ]

        print(f"æ­£åœ¨ç”Ÿæˆä»£ç  (æ¨¡å¼: {prompt_file}, Inputé•¿åº¦: {len(input_text)})...")
        
        # 3. è°ƒç”¨ LLM
        response = self.llm.chat(messages, system_prompt=system_prompt)

        # 4. æ¸…æ´—ä»£ç  (ç§»é™¤ Markdown æ ‡è®°)
        return self._clean_code(response)

    def generate_code_stream(self, input_text: str, prompt_file: str = "flowchart.md"):
        """
        ã€æµå¼ç”ŸæˆæŽ¥å£ã€‘æ”¯æŒæ‰“å­—æœºæ•ˆæžœ
        é€»è¾‘ä¸Ž generate_code å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯æ”¹ä¸º yield è¾“å‡º
        """
        # 1. åŠ è½½ Prompt (å¤ç”¨åŽŸæœ‰é€»è¾‘)
        system_prompt = self._load_system_prompt(prompt_file)
        
        # 2. æž„å»ºæ¶ˆæ¯
        messages: List[Message] = [
            {"role": "user", "content": f"[Requirements or content]:\n{input_text}"}
        ]

        print(f"ðŸŒŠ [CodeGen] æ­£åœ¨æµå¼ç”Ÿæˆä»£ç  (æ¨¡å¼: {prompt_file})...")
        
        # 3. è°ƒç”¨åº•å±‚çš„æµå¼æŽ¥å£ (yield)
        # æ³¨æ„ï¼šè¿™é‡Œç›´æŽ¥æŠŠ LLM çš„åŽŸå§‹ token åå‡ºæ¥ï¼Œä¸åš _clean_code æ¸…æ´—
        # å› ä¸ºæµå¼è¿‡ç¨‹ä¸­å¾ˆéš¾åˆ¤æ–­ ``` ä»€ä¹ˆæ—¶å€™ç»“æŸï¼Œæ¸…æ´—å·¥ä½œäº¤ç»™ API å±‚æˆ–å‰ç«¯å¤„ç†
        for chunk in self.llm.chat_stream(messages, system_prompt=system_prompt):
            if chunk:
                yield chunk
        
    def _clean_code(self, text: str) -> str:
        """å†…éƒ¨å·¥å…·ï¼šç§»é™¤ markdown ä»£ç å—æ ‡è®°ï¼Œæå–çº¯ä»£ç """
        text = text.strip()
        
        # ç§»é™¤å¸¸è§çš„ markdown ä»£ç å—å¤´éƒ¨
        # è¿™é‡Œä¸ä»…åŒ…å« mermaidï¼Œä¹Ÿé¢„ç•™äº† python ç­‰å…¶ä»–æ ‡è®°
        prefixes = ["```mermaid", "```python", "```javascript", "```xml", "```json", "```"]
        
        for prefix in prefixes:
            if text.startswith(prefix):
                text = text[len(prefix):]
                break # åªè¦åŒ¹é…åˆ°ä¸€ä¸ªå‰ç¼€å°±è·³å‡º
        
        # ç§»é™¤ç»“å°¾çš„ ```
        if text.endswith("```"):
            text = text[:-3]
            
        return text.strip()
    def reload_llm_config(self, config: dict):
        """
        ã€çƒ­æ›´æ–°ã€‘æŽ¥æ”¶å‰ç«¯é…ç½®(é©¼å³°å‘½å)å¹¶æ›´æ–°åº•å±‚ LLM
        """
        # ä»Žå­—å…¸ä¸­æå–é…ç½®
        api_key = config.get("apiKey")
        api_url = config.get("apiUrl")
        model_name = config.get("modelName")
        
        # è°ƒç”¨åº•å±‚ Agent.py ä¸­å®šä¹‰çš„ update_config
        # è¿™é‡Œçš„ self.llm å¯¹åº” Agent/deepseek_agent å®žä¾‹
        if hasattr(self, 'llm'):
            self.llm.update_config(api_key=api_key, base_url=api_url, model_name=model_name)
            print(f"ðŸ”„ [{self.__class__.__name__}] LLMé…ç½®å·²é‡è½½ -> æ¨¡åž‹: {model_name}")
