
import os
import re
import time
import requests
import hashlib
import json
import base64
import random
from concurrent.futures import ThreadPoolExecutor

# --- ä½ çš„ç°æœ‰æ¨¡å— ---
import utils
from code_revise import CodeReviseAgent

# ================= æš´åŠ›é…ç½®åŒº =================
# âš ï¸ ä¾ç„¶éœ€è¦ä½ çš„ Token
GITHUB_TOKEN = "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" 

# ä¿å­˜è·¯å¾„
SAVE_DIR = "./knowledge/mermaid_code"
MISTAKE_DB = "./knowledge/experience/mistakes.json"

# ã€å…³é”®å‡çº§ã€‘å…¨å¥— Mermaid æ ¸å¿ƒè¯­æ³•å…³é”®è¯ (åŸºäºä½ çš„å›¾ç‰‡å’Œå®˜æ–¹æ–‡æ¡£)
# åªè¦ä»£ç ä¸­ä¸åŒ…å«è¿™äº›è¯ä¸­çš„ä»»ä½•ä¸€ä¸ªï¼Œç›´æ¥è§†ä¸ºåƒåœ¾æ•°æ®ä¸¢å¼ƒ
VALID_KEYWORDS = [
    # åŸºç¡€æµç¨‹å›¾
    "graph", "flowchart", 
    # æ—¶åºå›¾ä¸ç±»å›¾
    "sequencediagram", "classdiagram", 
    # çŠ¶æ€ä¸å…³ç³»å›¾
    "statediagram", "statediagram-v2", "erdiagram", 
    # ç”¨æˆ·æ—…ç¨‹ä¸ç”˜ç‰¹å›¾
    "journey", "gantt", 
    # é¥¼å›¾ä¸è±¡é™å›¾
    "pie", "quadrantchart", 
    # éœ€æ±‚å›¾ä¸ Gitå›¾
    "requirementdiagram", "gitgraph", 
    # C4 æ¶æ„å›¾
    "c4context", "c4container", "c4component",
    # æ€ç»´å¯¼å›¾ä¸æ—¶é—´è½´
    "mindmap", "timeline", 
    # å®éªŒæ€§/æ–°ç‰¹æ€§ (å¯¹åº”ä½ å›¾ç‰‡é‡Œçš„ architecture, block, packet ç­‰)
    "zenuml", "sankey-beta", "sankey", "xychart-beta", "xychart",
    "block-beta", "block", "packet-beta", "packet", 
    "kanban", "architecture-beta", "architecture", "treemap"
]

# æœç´¢ç­–ç•¥ï¼šç›´æ¥é’ˆå¯¹è¿™äº›é«˜çº§å›¾è¡¨è¿›è¡Œæœç´¢
SEARCH_QUERIES = [
    "extension:mmd",                 # åŸºç¡€ç›˜
    "filename:*.md mermaid",         # å¹¿æ’’ç½‘
    # é’ˆå¯¹ç¨€æœ‰å›¾è¡¨çš„å®šå‘çˆ†ç ´
    "filename:*.md architecture-beta",
    "filename:*.md packet-beta",
    "filename:*.md block-beta",
    "filename:*.md kanban",
    "filename:*.md xychart",
    "filename:*.md c4context",
    "filename:*.md mindmap",
    "filename:*.md timeline",
    "filename:*.md zenuml",
]

# ç¿»é¡µæ·±åº¦ (æš´åŠ›æ¨¡å¼)
MAX_PAGES = 5  
PER_PAGE = 100 
# ==========================================

class GitHubMiner:
    def __init__(self, token, save_dir):
        self.token = token
        self.save_dir = save_dir
        self.headers = {
            "Authorization": f"token {self.token}",
            "Accept": "application/vnd.github.v3+json"
        }
        os.makedirs(self.save_dir, exist_ok=True)
        
        self.reviser = CodeReviseAgent(
            knowledge_base_dir="./knowledge_base",
            mistake_file_path=MISTAKE_DB
        )
        self.session = requests.Session()

    def _check_rate_limit(self, response):
        """é˜²å¾¡æœºåˆ¶ï¼šAPI ä½™é¢ä¸è¶³æ—¶å¼ºåˆ¶ä¼‘çœ """
        remaining = int(response.headers.get("x-ratelimit-remaining", 10))
        reset_time = int(response.headers.get("x-ratelimit-reset", 0))
        
        if remaining < 5:
            now = int(time.time())
            sleep_time = reset_time - now + 5
            if sleep_time > 0:
                print(f"\nâš ï¸ [è§¦å‘ç†”æ–­] API é¢åº¦è€—å°½ï¼æ­£åœ¨ä¼‘çœ  {sleep_time} ç§’...")
                time.sleep(sleep_time)
                print("â™»ï¸ æ»¡è¡€å¤æ´»ï¼Œç»§ç»­æŒ–æ˜ï¼")

    def search_github_aggressive(self, query):
        """ã€æš´åŠ›æ¨¡å¼ã€‘åˆ†é¡µæœç´¢"""
        url = "https://api.github.com/search/code"
        all_items = []
        
        print(f"\nğŸ’£ æ­£åœ¨è½°ç‚¸æœç´¢è¯: [{query}]")
        
        for page in range(1, MAX_PAGES + 1):
            params = {"q": query, "per_page": PER_PAGE, "page": page}
            
            try:
                while True:
                    resp = self.session.get(url, headers=self.headers, params=params)
                    
                    if resp.status_code == 200:
                        items = resp.json().get("items", [])
                        if not items:
                            print(f"   -> ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µã€‚")
                            return all_items
                        
                        print(f"   -> ç¬¬ {page} é¡µ: æ•è· {len(items)} ä¸ªç›®æ ‡")
                        all_items.extend(items)
                        time.sleep(2.5) # æ‰‹åŠ¨é™é€Ÿé˜²æ­¢ 403
                        break 
                        
                    elif resp.status_code == 403:
                        retry_after = int(resp.headers.get("Retry-After", 60))
                        print(f"   ğŸš« æœç´¢è¿‡çƒ­ (403)ï¼Œå†·å´ {retry_after} ç§’...")
                        time.sleep(retry_after + 2)
                        continue
                    else:
                        print(f"   âŒ æœç´¢å‡ºé”™: {resp.status_code}")
                        return all_items

            except Exception as e:
                print(f"   âŒ ç½‘ç»œå¼‚å¸¸: {e}")
                time.sleep(5)

        return all_items

    def _is_valid_mermaid_content(self, code):
        """
        ã€æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ã€‘
        æ­£åˆ™æå–å‡ºæ¥çš„å†…å®¹ï¼Œå¿…é¡»åŒ…å« VALID_KEYWORDS ä¸­çš„è‡³å°‘ä¸€ä¸ªã€‚
        ä¸åŒºåˆ†å¤§å°å†™ã€‚
        """
        code_lower = code.lower()
        
        # 1. é•¿åº¦æ£€æŸ¥ï¼šå¤ªçŸ­çš„è‚¯å®šä¸æ˜¯æ­£ç»å›¾
        if len(code.strip()) < 10:
            return False
            
        # 2. å…³é”®è¯å‘½ä¸­æ£€æŸ¥ (The "Accept Full Set or Reject" Logic)
        for kw in VALID_KEYWORDS:
            if kw in code_lower:
                return True
                
        return False

    def download_and_extract(self, item):
        """ä¸‹è½½å¹¶æå–"""
        file_url = item.get("url")
        path = item.get("path")
        
        try:
            resp = self.session.get(file_url, headers=self.headers)
            self._check_rate_limit(resp)
            
            if resp.status_code != 200: return 0
            
            content_json = resp.json()
            if "content" not in content_json: return 0
                
            raw_content = base64.b64decode(content_json["content"]).decode('utf-8', errors='ignore')
            extracted_codes = []
            
            # ç­–ç•¥ A: .mmd æ–‡ä»¶ (ç›´æ¥è§†ä¸ºä»£ç ï¼Œä½†ä»éœ€è¿‡å…³é”®è¯æ£€æŸ¥)
            if path.endswith(".mmd"):
                extracted_codes.append(raw_content)
                
            # ç­–ç•¥ B: .md æ–‡ä»¶ (æ­£åˆ™æå– ```mermaid ... ```)
            else:
                # æ­£åˆ™ï¼šå¼ºåˆ¶åŒ¹é… ```mermaid (å†…å®¹) ```
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ”¾å®½äº† mermaid åé¢å¯èƒ½è·Ÿçš„å­—ç¬¦ï¼Œåªè¦åœ¨ ``` å—å†…å³å¯
                pattern = r"```\s*mermaid\s*\n(.*?)\n\s*```"
                matches = re.findall(pattern, raw_content, re.DOTALL | re.IGNORECASE)
                extracted_codes.extend(matches)

            count = 0
            for code in extracted_codes:
                code = code.strip()
                
                # ã€ä¸¥æ ¼è¿‡æ»¤ã€‘æ‰¾ä¸åˆ°å…³é”®è¯ç›´æ¥åºŸé™¤
                if not self._is_valid_mermaid_content(code):
                    continue
                
                # å“ˆå¸Œå»é‡
                file_hash = hashlib.md5(code.encode('utf-8')).hexdigest()
                save_path = os.path.join(self.save_dir, f"{file_hash}.mmd")
                
                if not os.path.exists(save_path):
                    with open(save_path, "w", encoding="utf-8") as f:
                        f.write(code)
                    count += 1
            return count

        except Exception as e:
            return 0

    def verify_and_learn(self):
        """é—­ç¯å­¦ä¹ ä¸æ¸…ç†"""
        files = [f for f in os.listdir(self.save_dir) if f.endswith(".mmd")]
        total = len(files)
        if total == 0: return

        print(f"\nğŸ“ å¯åŠ¨æ‰¹é‡å®¡é˜… (åº“å­˜: {total} ä¸ªæ–‡ä»¶)...")
        stats = {"valid": 0, "fixed": 0, "deleted": 0}
        
        for i, filename in enumerate(files):
            if i % 10 == 0: print(f"   ...è¿›åº¦ {i}/{total}")
            
            file_path = os.path.join(self.save_dir, filename)
            with open(file_path, "r", encoding="utf-8") as f:
                original_code = f.read()
            
            # 1. æ ¡éªŒ
            res = utils.quick_validate_mermaid(original_code)
            if res['valid']:
                stats["valid"] += 1
                continue
            
            # 2. å°è¯•ä¿®å¤
            fixed_code = self.reviser.revise_code(original_code, error_message=res['error'])
            
            # 3. äºŒæ¬¡æ ¡éªŒ
            retry_res = utils.quick_validate_mermaid(fixed_code)
            if retry_res['valid']:
                stats["fixed"] += 1
                try:
                    self.reviser.record_mistake(original_code, res['error'], fixed_code)
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(fixed_code)
                except: pass
            else:
                stats["deleted"] += 1
                os.remove(file_path) 

        print(f"\nğŸ“Š æŒ–æ˜æŠ¥å‘Š:")
        print(f"   âœ… åŸç”Ÿä¼˜è´¨: {stats['valid']}")
        print(f"   ğŸ”§ ä¿®å¤æŒ½å›: {stats['fixed']}")
        print(f"   ğŸ—‘ï¸ åˆ é™¤åºŸæ–™: {stats['deleted']}")
        print(f"   ğŸ’° å½“å‰åº“å­˜: {len(os.listdir(self.save_dir))} ä¸ªé«˜è´¨é‡ç‰‡æ®µ")

    def run(self):
        print("ğŸš€ GitHub Mermaid æ•°æ®æŒ–æ˜æœº (Full-Spectrum Mode) å¯åŠ¨...")
        print(f"   ğŸ¯ ç›®æ ‡: å…¨é‡ Mermaid è¯­æ³• ({len(VALID_KEYWORDS)} ç§å…³é”®è¯)")
        
        total_extracted = 0
        
        for query in SEARCH_QUERIES:
            items = self.search_github_aggressive(query)
            if not items: continue
            
            print(f"   ğŸ“¥ å¼€å§‹ä¸‹è½½è§£æ {len(items)} ä¸ªæ–‡ä»¶...")
            with ThreadPoolExecutor(max_workers=5) as executor:
                results = list(executor.map(self.download_and_extract, items))
                count = sum(results)
                total_extracted += count
                print(f"   -> æœ¬è½®å…¥åº“: {count} ç‰‡æ®µ")
        
        print(f"\nâœ… æŒ–æ˜ç»“æŸï¼Œç´¯è®¡è·å– {total_extracted} ä¸ªæ–°ç‰‡æ®µã€‚")
        
        if total_extracted > 0:
            self.verify_and_learn()

if __name__ == "__main__":
    if "xx" in GITHUB_TOKEN:
        print("âŒ åˆ«æ€¥ç€è·‘ï¼å…ˆæŠŠ Token å¡«è¿›å»ï¼")
    else:
        miner = GitHubMiner(GITHUB_TOKEN, SAVE_DIR)
        miner.run()
