import os
import glob
import json
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm # å»ºè®®å®‰è£…: pip install tqdmï¼Œå¦‚æœæ²¡æœ‰å®‰è£…ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨é™çº§å¤„ç†

# --- å¼•ç”¨ä½ çš„ Agent ---
# å‡è®¾ Agent.py åœ¨åŒçº§ç›®å½•
from Agent import deepseek_agent

# ================= é…ç½®åŒº =================
RAW_DATA_DIR = "./knowledge/mermaid_code"
EXPERIENCE_DB = "./knowledge/experience/router.json"
MODEL_NAME = "deepseek-chat" # ä½¿ç”¨ Chat æ¨¡å‹å³å¯ï¼Œæˆæœ¬ä½é€Ÿåº¦å¿«

# è¿‡æ»¤é˜ˆå€¼
MIN_LINES = 5
MAX_LINES = 100
# ==========================================

class DataRefinery:
    def __init__(self):
        print(f"--- åˆå§‹åŒ– DataRefinery [æ¨¡å‹: {MODEL_NAME}] ---")
        self.llm = deepseek_agent(model_name=MODEL_NAME)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(EXPERIENCE_DB), exist_ok=True)
        
        # åŠ è½½ç°æœ‰ç»éªŒåº“ (é˜²æ­¢é‡å¤ç‚¼ä¸¹)
        self.existing_hashes = set()
        if os.path.exists(EXPERIENCE_DB):
            try:
                with open(EXPERIENCE_DB, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data:
                        # è®¡ç®—æºç çš„ hash ç”¨äºå»é‡
                        if "source_code" in item:
                            code_hash = hashlib.md5(item["source_code"].strip().encode()).hexdigest()
                            self.existing_hashes.add(code_hash)
                print(f"ğŸ“š å·²åŠ è½½ç°æœ‰ç»éªŒåº“: {len(self.existing_hashes)} æ¡ç»éªŒ")
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰ç»éªŒåº“å¤±è´¥: {e}ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
                self.existing_hashes = set()

    def _count_lines(self, file_path):
        """å¿«é€Ÿç»Ÿè®¡è¡Œæ•°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return sum(1 for _ in f)
        except:
            return 0

    def _analyze_single_file(self, file_path):
        """
        æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šè¯»å– -> æ ¡éªŒ -> LLMåˆ†æ -> è¿”å›ç»“æ„åŒ–æ•°æ®
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read().strip()
            
            # 1. åŸºç¡€å“ˆå¸Œå»é‡ check
            code_hash = hashlib.md5(code.encode()).hexdigest()
            if code_hash in self.existing_hashes:
                return None # è·³è¿‡å·²å­˜åœ¨çš„

            # 2. æ„é€  Prompt
            # æˆ‘ä»¬è¦æ±‚ LLM åšä¸¤ä»¶äº‹ï¼šé‰´åˆ«çœŸä¼ª + æç‚¼ç»éªŒ
            system_prompt = (
                "You are a Mermaid Code Analyst. Your task is to analyze the provided code snippet.\n"
                "1. **Validation**: Check if it is valid Mermaid code. (Ignore minor syntax errors, focus on structure).\n"
                "2. **Extraction**: If valid, extract the 'Scenario' (What is this graph about?) and 'Design Strategy' (Why use this chart type? Layout? Key features?).\n\n"
                "Output STRICT JSON format:\n"
                "{\n"
                "  \"is_mermaid\": true/false,\n"
                "  \"q\": \"Brief description of the content/scenario (e.g., User Login Flow)\",\n"
                "  \"a\": \"Brief explanation of design choices (e.g., Used SequenceDiagram to show time-ordered interactions...)\"\n"
                "}"
            )

            user_msg = f"Code snippet:\n```mermaid\n{code}\n```"
            
            # 3. è°ƒç”¨ LLM
            response = self.llm.chat(
                [{"role": "user", "content": user_msg}], 
                system_prompt=system_prompt, 
                json_mode=True
            )
            
            result = json.loads(response)
            
            # 4. ç»“æœå¤„ç†
            if result.get("is_mermaid") is True:
                # æˆåŠŸæç‚¼
                return {
                    "q": result.get("q", "Unknown Scenario"),
                    "a": result.get("a", "Standard Layout"),
                    "source_code": code # æŒ‰ç…§è¦æ±‚ï¼Œä¿ç•™æºç ä½œä¸ºæ¡ˆåº•
                }
            else:
                # LLM è®¤ä¸ºè¿™ä¸æ˜¯ Mermaid ä»£ç  (å¯èƒ½æ˜¯è¯¯çˆ¬çš„ markdown æ–‡æœ¬)
                return "INVALID"

        except Exception as e:
            # print(f"Error processing {file_path}: {e}")
            return None

    def run(self):
        print(f"ğŸš€ å¼€å§‹æ‰«æç›®å½•: {RAW_DATA_DIR}")
        all_files = glob.glob(os.path.join(RAW_DATA_DIR, "*.mmd"))
        
        # 1. ç¬¬ä¸€è½®è¿‡æ»¤ï¼šç¡¬è§„åˆ™ (è¡Œæ•°)
        candidates = []
        for p in all_files:
            lines = self._count_lines(p)
            if MIN_LINES <= lines <= MAX_LINES:
                candidates.append(p)
        
        print(f"ğŸ” æ‰«æåˆ° {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œç»è¡Œæ•°è¿‡æ»¤({MIN_LINES}-{MAX_LINES})åå‰©ä½™ {len(candidates)} ä¸ªå€™é€‰ã€‚")
        
        new_experiences = []
        invalid_count = 0
        skipped_count = 0
        
        # 2. ç¬¬äºŒè½®ï¼šå¹¶å‘ LLM æç‚¼
        # æ ¹æ®ä½ çš„ API é¢åº¦è°ƒæ•´ max_workersï¼ŒDeepSeek é€šå¸¸ 5-10 å¹¶å‘æ²¡é—®é¢˜
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤ä»»åŠ¡
            future_to_file = {executor.submit(self._analyze_single_file, fp): fp for fp in candidates}
            
            # è¿›åº¦æ¡å¤„ç†
            try:
                iterator = tqdm(as_completed(future_to_file), total=len(candidates), desc="ç‚¼ä¸¹è¿›åº¦")
            except ImportError:
                iterator = as_completed(future_to_file)
                print("æç¤º: å®‰è£… tqdm å¯æ˜¾ç¤ºè¿›åº¦æ¡ (pip install tqdm)")

            for future in iterator:
                res = future.result()
                
                if res == "INVALID":
                    invalid_count += 1
                elif res is None:
                    skipped_count += 1
                else:
                    new_experiences.append(res)
                    # å®æ—¶å†™å…¥å“ˆå¸Œé˜²æ­¢æœ¬æ¬¡è¿è¡Œé‡å¤ (è™½ç„¶ glob ä¸ä¼šé‡ï¼Œä½†ä¸ºäº†é€»è¾‘ä¸¥è°¨)
                    code_hash = hashlib.md5(res['source_code'].encode()).hexdigest()
                    self.existing_hashes.add(code_hash)

        print(f"\nğŸ“Š ç‚¼ä¸¹æŠ¥å‘Š:")
        print(f"   âœ… æ–°å¢ç»éªŒ: {len(new_experiences)} æ¡")
        print(f"   ğŸš« è¿‡æ»¤æ— æ•ˆ: {invalid_count} æ¡")
        print(f"   â­ï¸ è·³è¿‡é‡å¤: {skipped_count} æ¡")

        # 3. ç»“æœä¿å­˜
        if new_experiences:
            self._save_to_json(new_experiences)
        else:
            print("æ²¡æœ‰æå–åˆ°æ–°ç»éªŒã€‚")

    def _save_to_json(self, new_items):
        """å°†æ–°ç»éªŒè¿½åŠ åˆ° JSON æ–‡ä»¶"""
        final_data = []
        
        # è¯»å–æ—§æ•°æ®
        if os.path.exists(EXPERIENCE_DB):
            try:
                with open(EXPERIENCE_DB, 'r', encoding='utf-8') as f:
                    final_data = json.load(f)
            except:
                final_data = []
        
        # è¿½åŠ æ–°æ•°æ®
        final_data.extend(new_items)
        
        # å†™å…¥
        with open(EXPERIENCE_DB, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ ç»éªŒæ± å·²æ›´æ–°ï¼Œå½“å‰æ€»å®¹é‡: {len(final_data)} æ¡ã€‚")
        print(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„: {EXPERIENCE_DB}")

if __name__ == "__main__":
    refinery = DataRefinery()
    refinery.run()