from abc import ABC, abstractmethod
from typing import List, Dict, Generator, Union, Optional
from openai import OpenAI
import os
from pathlib import Path  # å¿…é¡»å¼•å…¥ Path

# --- 1. å®šä¹‰æ•°æ®ç»“æž„ ---
Message = Dict[str, str]

# --- 2. æŠ½è±¡æŽ¥å£ (Interface) ---
class abc_agent(ABC):
    @abstractmethod
    def chat(self, messages: List[Message], system_prompt: Optional[str] = None) -> str:
        pass

    @abstractmethod
    def chat_stream(self, messages: List[Message], system_prompt: Optional[str] = None) -> Generator[str, None, None]:
        pass

# --- 3. é€šç”¨ OpenAI åè®®å®žçŽ° ---
class Agent(abc_agent):
    """
    é€šç”¨ Agentï¼Œç”¨äºŽ DeepSeek ç­‰æ ‡å‡†æ¨¡åž‹
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, temperature: float = 0.7):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name
        self.temperature = temperature

    def chat(self, messages: List[Message], system_prompt: Optional[str] = None, json_mode: bool = False) -> str:
        final_msgs = []
        if system_prompt:
            final_msgs.append({"role": "system", "content": system_prompt})
        final_msgs.extend(messages)
        
        params = {
            "model": self.model_name,
            "messages": final_msgs,
            "temperature": self.temperature,
            "stream": False
        }
        
        # æ ‡å‡† OpenAI æ¨¡åž‹æ”¯æŒ json_object
        if json_mode:
            params["response_format"] = {"type": "json_object"}

        try:
            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content
        except Exception as e:
            return f"{{\"error\": \"Error invoking model {self.model_name}: {str(e)}\"}}"

    def chat_stream(self, messages: List[Message], system_prompt: Optional[str] = None) -> Generator[str, None, None]:
        final_msgs = []
        if system_prompt:
            final_msgs.append({"role": "system", "content": system_prompt})
        final_msgs.extend(messages)
        
        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=final_msgs,
                temperature=self.temperature,
                stream=True
            )
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"Error in stream: {str(e)}"

# --- DeepSeek Config ---
API_KEY_deepseek = "sk-53ad620095534cae927007367eecf082" 
BASE_URL_deepseek = "https://api.deepseek.com"

class deepseek_agent(Agent):
    def __init__(self, model_name="deepseek-chat", api_key=None, base_url=None):
        final_key = api_key if api_key else API_KEY_deepseek
        final_url = base_url if base_url else BASE_URL_deepseek
        super().__init__(final_key, final_url, model_name, 0.0)


# --- Qwen Config (ä¿®å¤ç‰ˆ) ---
API_KEY_qwen = "sk-3b009784a72d4d969c005e2afb2a7087"

class qwen_doc_agent:
    """
    ä¸“ç”¨äºŽ Qwen-Long çš„ Agent
    ä¿®å¤è¯´æ˜Žï¼šå°† fileid å’Œ system_prompt æ‹†åˆ†ä¸ºä¸¤æ¡æ¶ˆæ¯ï¼Œé¿å… 400 Invalid File é”™è¯¯
    """
    def __init__(self, model_name="qwen-long"):
        self.client = OpenAI(
            api_key=API_KEY_qwen,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model_name = model_name

    def chat(self, messages, system_prompt=None, file_path=None, json_mode=False):
        try:
            final_messages = []
            
            # --- 1. æ–‡ä»¶å¤„ç† ---
            if file_path:
                print(f"ðŸ“¤ [QwenAgent] Uploading: {file_path} ...")
                file_object = self.client.files.create(
                    file=Path(file_path),
                    purpose="file-extract"
                )
                file_id = file_object.id
                
                # å…³é”®ä¿®å¤ï¼šä½œä¸ºç‹¬ç«‹çš„ä¸€æ¡ system æ¶ˆæ¯å‘é€ fileid
                # è¿™æ ·åŽç«¯è§£æžæ—¶å°±ä¸ä¼šæŠŠåŽé¢çš„ prompt è¯¯è®¤ä¸ºæ˜¯ id çš„ä¸€éƒ¨åˆ†äº†
                final_messages.append({"role": "system", "content": f"fileid://{file_id}"})
                
                # å¦‚æžœè¿˜æœ‰é¢å¤–çš„ system_promptï¼Œä½œä¸ºç¬¬äºŒæ¡ system æ¶ˆæ¯è¿½åŠ 
                if system_prompt:
                    final_messages.append({"role": "system", "content": system_prompt})
            
            # æ²¡æœ‰æ–‡ä»¶ï¼Œåªæœ‰ prompt çš„æƒ…å†µ
            elif system_prompt:
                final_messages.append({"role": "system", "content": system_prompt})

            # 2. è¿½åŠ ç”¨æˆ·æ¶ˆæ¯
            final_messages.extend(messages)

            # 3. å‡†å¤‡å‚æ•°
            params = {
                "model": self.model_name,
                "messages": final_messages,
                "stream": False
            }
            
            # json_mode å¯¹ qwen-long æš‚æ—¶ä¸åŠ  response_format ä»¥é˜²å…¼å®¹æ€§é—®é¢˜

            # 4. è°ƒç”¨
            response = self.client.chat.completions.create(**params)
            content = response.choices[0].message.content
            
            # æ¸…æ´— markdown json æ ‡è®°
            if json_mode and "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif json_mode and "```" in content: # ç®€å•çš„ markdown æ¸…æ´—
                content = content.split("```")[1].split("```")[0].strip()
            
            return content

        except Exception as e:
            print(f"âŒ Qwen Agent Critical Error: {e}")
            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
            if hasattr(e, 'body'):
                print(f"   -> Error Body: {e.body}")
            return "{}"