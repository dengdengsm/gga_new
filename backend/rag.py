import os
import chromadb
import torch
import json
import uuid
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

class LocalKnowledgeBase:
    def __init__(self, persist_dir: str = "./.local_rag_db"):
        """
        åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“
        """
        print("--- åˆå§‹åŒ– RAG å¼•æ“ (æ”¯æŒ Q&A Key-Value æ¨¡å¼) ---")
        
        # 1. æ˜¾å­˜å¤Ÿå¤§ï¼Œç›´æ¥ä¸Š BAAI/bge-m3 (çº¦ 2.5GB)ï¼Œä¸­è‹±æ–‡æ•ˆæœé¡¶çº§
        # å¦‚æœä¸‹è½½æ…¢ï¼Œè¯·æå‰ä¸‹è½½å¥½æ¨¡å‹æ–‡ä»¶å¤¹ï¼ŒæŠŠä¸‹é¢çš„å­—ç¬¦ä¸²æ¢æˆè·¯å¾„
        model_name = "BAAI/bge-m3"
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ ({model_name}) åˆ° {device}...")
        
        self.encoder = SentenceTransformer(model_name, device=device)
        
        # 2. åˆå§‹åŒ–æœ¬åœ°å‘é‡æ•°æ®åº“ (ChromaDB)
        self.client = chromadb.PersistentClient(path=persist_dir)
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        # hnsw:space='cosine' è¡¨ç¤ºä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæœ€é€‚åˆæ–‡æœ¬åŒ¹é…
        self.collection = self.client.get_or_create_collection(
            name="general_knowledge",
            metadata={"hnsw:space": "cosine"}
        )
        print("å¼•æ“å°±ç»ªã€‚")

    def add_markdown(self, file_path: str):
        """
        æ ¸å¿ƒåŠŸèƒ½1ï¼šè¯»å– MD æ–‡ä»¶ -> æ™ºèƒ½åˆ‡åˆ† -> å‘é‡åŒ–å…¥åº“
        """
        if not os.path.exists(file_path):
            # å®¹é”™å¤„ç†ï¼šå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»…æ‰“å°è­¦å‘Šï¼Œä¸ä¸­æ–­ç¨‹åº
            print(f"è­¦å‘Š: æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            return

        print(f"æ­£åœ¨å¤„ç†è§„åˆ™æ–‡æ¡£: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # --- ç¬¬ä¸€æ­¥ï¼šæŒ‰ Markdown æ ‡é¢˜å±‚çº§åˆ‡åˆ† (ä¿ç•™ç»“æ„) ---
        # è¿™æ ·èƒ½ä¿è¯æ£€ç´¢åˆ°â€œä»£ç è§„åˆ™â€æ—¶ï¼ŒçŸ¥é“å®ƒæ˜¯å±äºå“ªä¸ªå¤§ç±»çš„
        headers = [
            ("#", "H1"),
            ("##", "H2"),
            ("###", "H3"),
        ]
        md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)
        header_splits = md_splitter.split_text(text)

        # --- ç¬¬äºŒæ­¥ï¼šæŒ‰å­—ç¬¦é•¿åº¦äºŒæ¬¡åˆ‡åˆ† (é˜²æ­¢å•æ®µè¿‡é•¿) ---
        # bge-m3 æ”¯æŒ 8192 é•¿åº¦ï¼Œä½†ä¸ºäº†æ£€ç´¢ç²¾å‡†ï¼Œå»ºè®®åˆ‡ç»†ä¸€ç‚¹ï¼Œæ¯”å¦‚ 512 æˆ– 1024
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=100, # é‡å ä¸€éƒ¨åˆ†ï¼Œé˜²æ­¢æ–­ç« å–ä¹‰
            separators=["\n\n", "\n", "ã€‚", "ï¼", "!", ""]
        )
        final_splits = text_splitter.split_documents(header_splits)

        # --- ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡å‘é‡åŒ–å¹¶å­˜å‚¨ ---
        documents = []
        metadatas = []
        ids = []

        base_name = os.path.basename(file_path)
        
        for idx, doc in enumerate(final_splits):
            documents.append(doc.page_content)
            # æŠŠæ–‡ä»¶æ¥æºè®°å½•åœ¨å…ƒæ•°æ®é‡Œ
            meta = doc.metadata.copy()
            meta["source"] = base_name
            meta["type"] = "doc_fragment"
            metadatas.append(meta)
            ids.append(f"{base_name}_part_{idx}")

        if documents:
            # normalize_embeddings=True å¯¹ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢éå¸¸é‡è¦
            embeddings = self.encoder.encode(documents, normalize_embeddings=True).tolist()
            
            self.collection.add(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas
            )
            print(f"æˆåŠŸå…¥åº“ {len(documents)} ä¸ªç‰‡æ®µã€‚")

    def add_qa_mistakes(self, json_path: str):
        """
        æ ¸å¿ƒåŠŸèƒ½2 (å‡çº§ç‰ˆ)ï¼šKey-Value RAG æ¨¡å¼
        è¯»å– JSON é”™é¢˜é›† -> Embedding(Q) -> Store(A)
        ç›®çš„ï¼šå½“ Query åŒ¹é…åˆ°æŠ¥é”™ä¿¡æ¯(Q)æ—¶ï¼Œç›´æ¥è¿”å›ä¿®å¤ç­–ç•¥(A)ï¼Œè€Œéæ— å…³æ–‡æœ¬ã€‚
        """
        if not os.path.exists(json_path):
            print(f"æç¤º: é”™é¢˜é›†æ–‡ä»¶ {json_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½ã€‚")
            return

        print(f"æ­£åœ¨åŠ è½½é”™é¢˜ç»éªŒ: {json_path}")
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if not isinstance(data, list):
                print("é”™é¢˜é›†æ ¼å¼é”™è¯¯: æ ¹èŠ‚ç‚¹åº”ä¸º List")
                return

            ids = []
            documents = []  # å­˜ Answer (ä¿®å¤ç­–ç•¥)
            embeddings = [] # å­˜ Question (æŠ¥é”™ä¿¡æ¯) çš„å‘é‡
            metadatas = []

            # æ‰¹é‡å¤„ç†ï¼Œå‡å°‘ç¼–ç æ¬¡æ•°
            questions = []
            
            for idx, item in enumerate(data):
                q = item.get("q", "")
                a = item.get("a", "")
                
                if not q or not a:
                    continue
                
                questions.append(q)
                documents.append(a)
                # è®°å½•åŸå§‹é—®é¢˜åœ¨ metadata ä¸­ï¼Œæ–¹ä¾¿å›æº¯
                metadatas.append({
                    "source": "mistakes_json", 
                    "type": "qa_experience",
                    "original_q": q
                })
                ids.append(f"mistake_{idx}_{str(uuid.uuid4())[:8]}")

            if questions:
                # æ ¸å¿ƒï¼šå‘é‡åŒ–çš„æ˜¯ Question (æŠ¥é”™ç‰¹å¾)
                embeddings = self.encoder.encode(questions, normalize_embeddings=True).tolist()
                
                self.collection.add(
                    ids=ids,
                    documents=documents, # æ£€ç´¢è¿”å›çš„å†…å®¹æ˜¯ Answer
                    embeddings=embeddings, # æ£€ç´¢åŒ¹é…çš„ä¾æ®æ˜¯ Question
                    metadatas=metadatas
                )
                print(f"æˆåŠŸåŠ è½½ {len(documents)} æ¡é”™é¢˜ç»éªŒã€‚")
                
        except Exception as e:
            print(f"åŠ è½½é”™é¢˜é›†å¤±è´¥: {str(e)}")

    def add_single_qa(self, q: str, a: str, source: str = "runtime_learning"):
        """
        è¿è¡Œæ—¶åŠ¨æ€æ·»åŠ å•æ¡ç»éªŒ (Experience Replay)
        """
        try:
            embedding = self.encoder.encode([q], normalize_embeddings=True).tolist()
            
            unique_id = f"runtime_mistake_{str(uuid.uuid4())[:8]}"
            
            self.collection.add(
                ids=[unique_id],
                documents=[a],
                embeddings=embedding,
                metadatas=[{"source": source, "type": "qa_experience", "original_q": q}]
            )
            print(f"å·²åŠ¨æ€è®°å½•ç»éªŒ: {q[:30]}...")
        except Exception as e:
            print(f"åŠ¨æ€è®°å½•ç»éªŒå¤±è´¥: {e}")

    def search(self, query: str, top_k: int = 3) -> List[str]:
        """
        [ä¿®æ­£ç‰ˆ] æ™ºèƒ½å»é‡æ£€ç´¢
        ä¿®æ­£ç‚¹ï¼šé’ˆå¯¹ QA æ•°æ®ï¼ŒåŸºäº Question (original_q) å»é‡ï¼Œé˜²æ­¢è¯¯åˆ ä¸åŒé”™è¯¯ä½†ä¿®å¤æ–¹æ¡ˆç›¸åŒçš„æ¡ç›®ã€‚
        """
        # 1. å‘é‡åŒ–
        query_vec = self.encoder.encode([query], normalize_embeddings=True).tolist()
        
        # 2. è¿‡é‡‡æ ·æ£€ç´¢
        results = self.collection.query(
            query_embeddings=query_vec,
            n_results=top_k * 3 
        )
        
        found_docs = results['documents'][0] if results['documents'] else []
        found_ids = results['ids'][0] if results['ids'] else []
        found_metadatas = results['metadatas'][0] if results['metadatas'] else []
        
        unique_docs = []
        seen_hashes = set()
        ids_to_delete = [] 

        # 3. æ™ºèƒ½å»é‡éå†
        for i in range(len(found_docs)):
            doc = found_docs[i]
            doc_id = found_ids[i]
            meta = found_metadatas[i] if found_metadatas else {}
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šåˆ¤é‡æŒ‡çº¹è®¡ç®— ---
            if meta and "original_q" in meta:
                # å¦‚æœæ˜¯ QA é”™é¢˜/ç»éªŒï¼Œå¿…é¡»åŸºäºâ€œé—®é¢˜(Q)â€æ¥åˆ¤é‡ï¼
                # åªæœ‰å½“â€œé—®é¢˜â€ä¸€æ¨¡ä¸€æ ·æ—¶ï¼Œæ‰è§†ä¸ºå†—ä½™æ•°æ®ã€‚
                unique_key = meta["original_q"].strip()
            else:
                # å¦‚æœæ˜¯æ™®é€šæ–‡æ¡£ï¼Œåˆ™åŸºäºâ€œå†…å®¹â€åˆ¤é‡
                unique_key = doc.strip()
                
            # è®¡ç®—æŒ‡çº¹
            import hashlib
            item_hash = hashlib.md5(unique_key.encode('utf-8')).hexdigest()
            
            if item_hash not in seen_hashes:
                unique_docs.append(doc)
                seen_hashes.add(item_hash)
            else:
                # æŒ‡çº¹é‡å¤ï¼Œè¯´æ˜åº“é‡Œæœ‰å†—ä½™çš„ Q (å¯¹äºé”™é¢˜) æˆ–å†—ä½™çš„æ–‡æœ¬ (å¯¹äºæ–‡æ¡£)
                ids_to_delete.append(doc_id)
            
            if len(unique_docs) >= top_k and len(ids_to_delete) == 0:
                break

        # 4. æ‰§è¡Œæ¸…ç†
        if ids_to_delete:
            # print(f"ğŸ§¹ [RAGæ¸…ç†] ç§»é™¤ {len(ids_to_delete)} æ¡å†—ä½™æ•°æ®...") # å‡å°‘æ—¥å¿—å¹²æ‰°
            try:
                self.collection.delete(ids=ids_to_delete)
            except: pass

        return unique_docs[:top_k]
    
    def search_score(self, query: str, top_k: int = 3, score_threshold: float = 0.4) -> List[str]:
        """
        [ä¿®æ­£ç‰ˆ] æ™ºèƒ½å»é‡ + é˜ˆå€¼æˆªæ–­æ£€ç´¢
        :param score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)ï¼Œä½äºæ­¤å€¼çš„ç»éªŒå°†è¢«å¿½ç•¥ã€‚å»ºè®® 0.35 ~ 0.5 ä¹‹é—´ã€‚
        """
        # 1. å‘é‡åŒ–
        query_vec = self.encoder.encode([query], normalize_embeddings=True).tolist()
        
        # 2. è¿‡é‡‡æ ·æ£€ç´¢ (ä¸ºäº†åœ¨è¿‡æ»¤å’Œå»é‡åè¿˜èƒ½å‡‘å¤Ÿ top_kï¼Œè¿™é‡Œå¤šå–ä¸€äº›)
        results = self.collection.query(
            query_embeddings=query_vec,
            n_results=top_k * 5, 
            # å¿…é¡»æ˜¾å¼è¯·æ±‚ distances
            include=["documents", "metadatas", "distances"] 
        )
        
        found_docs = results['documents'][0] if results['documents'] else []
        found_ids = results['ids'][0] if results['ids'] else []
        found_metadatas = results['metadatas'][0] if results['metadatas'] else []
        # è·å–è·ç¦»åˆ—è¡¨
        found_distances = results['distances'][0] if results['distances'] else []
        
        unique_docs = []
        seen_hashes = set()
        ids_to_delete = [] 

        # 3. æ™ºèƒ½å»é‡ + é˜ˆå€¼è¿‡æ»¤éå†
        for i in range(len(found_docs)):
            doc = found_docs[i]
            doc_id = found_ids[i]
            meta = found_metadatas[i] if found_metadatas else {}
            dist = found_distances[i]
            
            # --- [æ ¸å¿ƒä¿®æ”¹] ç›¸ä¼¼åº¦é˜ˆå€¼åˆ¤æ–­ ---
            # Chroma çš„ Cosine Distance èŒƒå›´æ˜¯ 0~2 (0è¡¨ç¤ºå®Œå…¨ä¸€æ ·)
            # ç›¸ä¼¼åº¦ = 1 - è·ç¦»
            similarity = 1.0 - dist
            
            if similarity < score_threshold:
                # å› ä¸º Chroma è¿”å›çš„ç»“æœæ˜¯æŒ‰ç›¸ä¼¼åº¦æ’åºçš„ (è·ç¦»ç”±å°åˆ°å¤§)
                # å¦‚æœå½“å‰è¿™æ¡å·²ç»ä½äºé˜ˆå€¼ï¼Œåé¢çš„è‚¯å®šæ›´ä½ï¼Œç›´æ¥ç»“æŸå¾ªç¯
                # print(f"   [RAGè¿‡æ»¤] ç›¸ä¼¼åº¦ {similarity:.4f} ä½äºé˜ˆå€¼ {score_threshold}ï¼Œæˆªæ–­åœæ­¢ã€‚")
                break

            # --- ä¸‹é¢æ˜¯åŸæœ¬çš„å»é‡é€»è¾‘ ---
            if meta and "original_q" in meta:
                unique_key = meta["original_q"].strip()
            else:
                unique_key = doc.strip()
                
            import hashlib
            item_hash = hashlib.md5(unique_key.encode('utf-8')).hexdigest()
            
            if item_hash not in seen_hashes:
                unique_docs.append(doc)
                seen_hashes.add(item_hash)
            else:
                ids_to_delete.append(doc_id)
            
            # å‡‘å¤Ÿäº†å°±åœ
            if len(unique_docs) >= top_k:
                break

        # 4. æ‰§è¡Œæ¸…ç† (ä¿æŒä¸å˜)
        if ids_to_delete:
            try:
                self.collection.delete(ids=ids_to_delete)
            except: pass

        return unique_docs